{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_for_image_captioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9l8CKtvNq2oh8+0/tOaYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicasjsu/deep_learning/blob/master/Model_for_image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEuxYPGOkS6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5Jm91eilN7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d821f7a5-0ff9-4445-c810-eb4a84f05ad4"
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 8s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 382s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmR1VfD5lXD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the json file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjM91MsQlZZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a82497e-f4ec-489e-822a-6e60dfb2d8fc"
      },
      "source": [
        "len(train_captions), len(all_captions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 414113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tSmfjV9noKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkofWb3unqYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e040b794-1cdb-40ed-ebab-d23c2a70a081"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLOJc_LVntiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eCW_CyxnxPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2EQeSZynzuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpniIqZ-n1uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWi9Law7n3vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4DB7RyGn7F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20LEqzKBn86d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItJhF0RLn-w9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and validation sets using an 80-20 split\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvZQONP1oCh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8debc79-ed8a-4cea-f708-eb1503e302cf"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LDBC2W5oPzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train)\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zInrxlVoT6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0axMhsqloVwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_mIpBZg9aN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV5hgyv69b6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GsREuta9f2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg85M0Gc9iMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqtnvccY9kBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8eVBUNg9l9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wNslWQu9nh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwTO_AWY9rrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_plot = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCcCp2Ho9trj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "  return loss, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zK8ucT9wz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dec5e528-7521-4461-e91d-37d7a94c7c39"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1045\n",
            "Epoch 1 Batch 100 Loss 1.0985\n",
            "Epoch 1 Batch 200 Loss 1.0190\n",
            "Epoch 1 Batch 300 Loss 0.9387\n",
            "Epoch 1 Batch 400 Loss 0.8584\n",
            "Epoch 1 Batch 500 Loss 0.7902\n",
            "Epoch 1 Batch 600 Loss 0.8629\n",
            "Epoch 1 Batch 700 Loss 0.7620\n",
            "Epoch 1 Loss 0.967539\n",
            "Time taken for 1 epoch 383.6107985973358 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7741\n",
            "Epoch 2 Batch 100 Loss 0.8106\n",
            "Epoch 2 Batch 200 Loss 0.7597\n",
            "Epoch 2 Batch 300 Loss 0.7762\n",
            "Epoch 2 Batch 400 Loss 0.6884\n",
            "Epoch 2 Batch 500 Loss 0.7429\n",
            "Epoch 2 Batch 600 Loss 0.6879\n",
            "Epoch 2 Batch 700 Loss 0.7010\n",
            "Epoch 2 Loss 0.763638\n",
            "Time taken for 1 epoch 363.24363112449646 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6640\n",
            "Epoch 3 Batch 100 Loss 0.7014\n",
            "Epoch 3 Batch 200 Loss 0.6589\n",
            "Epoch 3 Batch 300 Loss 0.6698\n",
            "Epoch 3 Batch 400 Loss 0.6667\n",
            "Epoch 3 Batch 500 Loss 0.6173\n",
            "Epoch 3 Batch 600 Loss 0.7698\n",
            "Epoch 3 Batch 700 Loss 0.7084\n",
            "Epoch 3 Loss 0.701228\n",
            "Time taken for 1 epoch 361.00651717185974 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6050\n",
            "Epoch 4 Batch 100 Loss 0.7233\n",
            "Epoch 4 Batch 200 Loss 0.7892\n",
            "Epoch 4 Batch 300 Loss 0.6078\n",
            "Epoch 4 Batch 400 Loss 0.6822\n",
            "Epoch 4 Batch 500 Loss 0.6134\n",
            "Epoch 4 Batch 600 Loss 0.6740\n",
            "Epoch 4 Batch 700 Loss 0.6972\n",
            "Epoch 4 Loss 0.657932\n",
            "Time taken for 1 epoch 359.4066479206085 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6258\n",
            "Epoch 5 Batch 100 Loss 0.6363\n",
            "Epoch 5 Batch 200 Loss 0.6541\n",
            "Epoch 5 Batch 300 Loss 0.6499\n",
            "Epoch 5 Batch 400 Loss 0.6106\n",
            "Epoch 5 Batch 500 Loss 0.5772\n",
            "Epoch 5 Batch 600 Loss 0.5751\n",
            "Epoch 5 Batch 700 Loss 0.6314\n",
            "Epoch 5 Loss 0.620825\n",
            "Time taken for 1 epoch 359.267053604126 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5951\n",
            "Epoch 6 Batch 100 Loss 0.6600\n",
            "Epoch 6 Batch 200 Loss 0.5898\n",
            "Epoch 6 Batch 300 Loss 0.6252\n",
            "Epoch 6 Batch 400 Loss 0.5905\n",
            "Epoch 6 Batch 500 Loss 0.4908\n",
            "Epoch 6 Batch 600 Loss 0.6773\n",
            "Epoch 6 Batch 700 Loss 0.5668\n",
            "Epoch 6 Loss 0.586415\n",
            "Time taken for 1 epoch 358.5806543827057 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6087\n",
            "Epoch 7 Batch 100 Loss 0.5827\n",
            "Epoch 7 Batch 200 Loss 0.5489\n",
            "Epoch 7 Batch 300 Loss 0.5893\n",
            "Epoch 7 Batch 400 Loss 0.5289\n",
            "Epoch 7 Batch 500 Loss 0.5393\n",
            "Epoch 7 Batch 600 Loss 0.5630\n",
            "Epoch 7 Batch 700 Loss 0.5190\n",
            "Epoch 7 Loss 0.553402\n",
            "Time taken for 1 epoch 358.6078140735626 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5257\n",
            "Epoch 8 Batch 100 Loss 0.5487\n",
            "Epoch 8 Batch 200 Loss 0.5651\n",
            "Epoch 8 Batch 300 Loss 0.4557\n",
            "Epoch 8 Batch 400 Loss 0.5111\n",
            "Epoch 8 Batch 500 Loss 0.5155\n",
            "Epoch 8 Batch 600 Loss 0.5000\n",
            "Epoch 8 Batch 700 Loss 0.5269\n",
            "Epoch 8 Loss 0.523277\n",
            "Time taken for 1 epoch 359.47229623794556 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4595\n",
            "Epoch 9 Batch 100 Loss 0.4961\n",
            "Epoch 9 Batch 200 Loss 0.6231\n",
            "Epoch 9 Batch 300 Loss 0.4919\n",
            "Epoch 9 Batch 400 Loss 0.5106\n",
            "Epoch 9 Batch 500 Loss 0.4597\n",
            "Epoch 9 Batch 600 Loss 0.4432\n",
            "Epoch 9 Batch 700 Loss 0.5281\n",
            "Epoch 9 Loss 0.489350\n",
            "Time taken for 1 epoch 359.148597240448 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5026\n",
            "Epoch 10 Batch 100 Loss 0.4832\n",
            "Epoch 10 Batch 200 Loss 0.4741\n",
            "Epoch 10 Batch 300 Loss 0.4466\n",
            "Epoch 10 Batch 400 Loss 0.4476\n",
            "Epoch 10 Batch 500 Loss 0.4114\n",
            "Epoch 10 Batch 600 Loss 0.4202\n",
            "Epoch 10 Batch 700 Loss 0.3830\n",
            "Epoch 10 Loss 0.459235\n",
            "Time taken for 1 epoch 359.22160720825195 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.4493\n",
            "Epoch 11 Batch 100 Loss 0.4642\n",
            "Epoch 11 Batch 200 Loss 0.3611\n",
            "Epoch 11 Batch 300 Loss 0.4280\n",
            "Epoch 11 Batch 400 Loss 0.4301\n",
            "Epoch 11 Batch 500 Loss 0.4318\n",
            "Epoch 11 Batch 600 Loss 0.4576\n",
            "Epoch 11 Batch 700 Loss 0.3876\n",
            "Epoch 11 Loss 0.428489\n",
            "Time taken for 1 epoch 358.9721796512604 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4278\n",
            "Epoch 12 Batch 100 Loss 0.3792\n",
            "Epoch 12 Batch 200 Loss 0.4487\n",
            "Epoch 12 Batch 300 Loss 0.3959\n",
            "Epoch 12 Batch 400 Loss 0.4797\n",
            "Epoch 12 Batch 500 Loss 0.3656\n",
            "Epoch 12 Batch 600 Loss 0.4106\n",
            "Epoch 12 Batch 700 Loss 0.4075\n",
            "Epoch 12 Loss 0.401202\n",
            "Time taken for 1 epoch 359.07355785369873 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3744\n",
            "Epoch 13 Batch 100 Loss 0.4092\n",
            "Epoch 13 Batch 200 Loss 0.3437\n",
            "Epoch 13 Batch 300 Loss 0.3427\n",
            "Epoch 13 Batch 400 Loss 0.3578\n",
            "Epoch 13 Batch 500 Loss 0.3666\n",
            "Epoch 13 Batch 600 Loss 0.3520\n",
            "Epoch 13 Batch 700 Loss 0.3710\n",
            "Epoch 13 Loss 0.372204\n",
            "Time taken for 1 epoch 358.2355692386627 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.4139\n",
            "Epoch 14 Batch 100 Loss 0.3584\n",
            "Epoch 14 Batch 200 Loss 0.3600\n",
            "Epoch 14 Batch 300 Loss 0.3315\n",
            "Epoch 14 Batch 400 Loss 0.3363\n",
            "Epoch 14 Batch 500 Loss 0.3405\n",
            "Epoch 14 Batch 600 Loss 0.3439\n",
            "Epoch 14 Batch 700 Loss 0.3188\n",
            "Epoch 14 Loss 0.346583\n",
            "Time taken for 1 epoch 358.5883128643036 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.3765\n",
            "Epoch 15 Batch 100 Loss 0.2714\n",
            "Epoch 15 Batch 200 Loss 0.3009\n",
            "Epoch 15 Batch 300 Loss 0.3618\n",
            "Epoch 15 Batch 400 Loss 0.3241\n",
            "Epoch 15 Batch 500 Loss 0.2950\n",
            "Epoch 15 Batch 600 Loss 0.2806\n",
            "Epoch 15 Batch 700 Loss 0.3512\n",
            "Epoch 15 Loss 0.322247\n",
            "Time taken for 1 epoch 358.1388840675354 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3552\n",
            "Epoch 16 Batch 100 Loss 0.2583\n",
            "Epoch 16 Batch 200 Loss 0.3111\n",
            "Epoch 16 Batch 300 Loss 0.2753\n",
            "Epoch 16 Batch 400 Loss 0.2554\n",
            "Epoch 16 Batch 500 Loss 0.2951\n",
            "Epoch 16 Batch 600 Loss 0.3244\n",
            "Epoch 16 Batch 700 Loss 0.2487\n",
            "Epoch 16 Loss 0.301129\n",
            "Time taken for 1 epoch 358.36342191696167 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3140\n",
            "Epoch 17 Batch 100 Loss 0.3033\n",
            "Epoch 17 Batch 200 Loss 0.2846\n",
            "Epoch 17 Batch 300 Loss 0.2598\n",
            "Epoch 17 Batch 400 Loss 0.2950\n",
            "Epoch 17 Batch 500 Loss 0.2813\n",
            "Epoch 17 Batch 600 Loss 0.2419\n",
            "Epoch 17 Batch 700 Loss 0.2696\n",
            "Epoch 17 Loss 0.280421\n",
            "Time taken for 1 epoch 358.1990211009979 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2761\n",
            "Epoch 18 Batch 100 Loss 0.2489\n",
            "Epoch 18 Batch 200 Loss 0.2461\n",
            "Epoch 18 Batch 300 Loss 0.2853\n",
            "Epoch 18 Batch 400 Loss 0.2685\n",
            "Epoch 18 Batch 500 Loss 0.2779\n",
            "Epoch 18 Batch 600 Loss 0.2062\n",
            "Epoch 18 Batch 700 Loss 0.2587\n",
            "Epoch 18 Loss 0.262738\n",
            "Time taken for 1 epoch 357.86155700683594 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2762\n",
            "Epoch 19 Batch 100 Loss 0.2650\n",
            "Epoch 19 Batch 200 Loss 0.2546\n",
            "Epoch 19 Batch 300 Loss 0.2822\n",
            "Epoch 19 Batch 400 Loss 0.2348\n",
            "Epoch 19 Batch 500 Loss 0.2201\n",
            "Epoch 19 Batch 600 Loss 0.2431\n",
            "Epoch 19 Batch 700 Loss 0.2586\n",
            "Epoch 19 Loss 0.244997\n",
            "Time taken for 1 epoch 357.623108625412 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2919\n",
            "Epoch 20 Batch 100 Loss 0.2412\n",
            "Epoch 20 Batch 200 Loss 0.2052\n",
            "Epoch 20 Batch 300 Loss 0.2341\n",
            "Epoch 20 Batch 400 Loss 0.2429\n",
            "Epoch 20 Batch 500 Loss 0.2299\n",
            "Epoch 20 Batch 600 Loss 0.2169\n",
            "Epoch 20 Batch 700 Loss 0.2149\n",
            "Epoch 20 Loss 0.231423\n",
            "Time taken for 1 epoch 357.71596670150757 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxA7Inoy90Mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d695cc54-71a4-4139-8190-0b9e207074ac"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}