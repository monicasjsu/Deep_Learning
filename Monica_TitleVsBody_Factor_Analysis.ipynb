{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monica_TitleVsBody_Factor_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicasjsu/deep_learning/blob/master/Monica_TitleVsBody_Factor_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cURC77eCYB97",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Title Vs Body**\n",
        "#### Monica Dommaraju (014545336)\n",
        "#### Team Name : Shecodes\n",
        "\n",
        "---\n",
        "\n",
        "### **Dataset Details**\n",
        "\n",
        "- Kaggle Fakenews Dataset\n",
        "- Liar Liar Dataset\n",
        "- Politifact Fake news and Real News Content\n",
        "- Stance Dataset from Fake News Challenge\n",
        "\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "During my first two sprints, I have been exploring datasets and doing research on how to generate features from title and text, how to amalgamate multiple datasets etc.\n",
        "\n",
        "I generated couple features releated to similarities between title and text using TF-IDF and Cosine similarities. I have calculated the count of overlapping strong words between title and text. \n",
        "\n",
        "Starting third sprint, I started adding more and more features using multiple techinques such as\n",
        "\n",
        " - Performed data cleaning on the title and text columns that include Removing symbols, punctuation, stop word removal, Lemmaization, Stemming, etc..\n",
        " - Generating n-grams (unigram, bigrams and trigrams)\n",
        " - Content statistics that include calculating the lenght of sentences, count of n-grams and count of overlapping n-grams between title and text.\n",
        " - I then simplified and enhanced my TD-IDF algorigthm and calculated cosine similaties between the title and text.\n",
        " - Used SVD to perform dimensionality reduction on TF-IDF vectors and to perform Latent Semantic Analysis (LSA) as part of topic modelling. \n",
        " - Performed cosine similarities between the title and text topics generated from above step.\n",
        " - Used Word2Vec using Google News corpus to find synonyms and next probable words on both title and text and calculated the similarites from the outcome.\n",
        " - Performed Sentiment analysis on both title and text was extracted the sentiment scores for (pos, neg, nuetral).\n",
        " - Performed normlization on all the distilled features using Standard Scaler. \n",
        " - Ran all the above steps on two datasets. One with multiclass label and the other on the binary classified amalgamated data set.\n",
        " - Ran Many different models on both the datasets. (Multi classification models and Binary classification models)\n",
        " - Ran Stratified KFold cross validation technique to make sure the model is not overfitting.\n",
        " - Analyed the results and derived a polynomical equation to generate the fakeness score by taking the predictions from both multi classification and binary classification models  \n",
        " - Randomly picked a few samples and tested them by inputting them to the prediction function which then generated the overall fakeness score.\n",
        " - I was able to acheive an overall accuracy of --\n",
        "\n",
        "### **What did I do and what worked?**\n",
        "I was able to do all the things that I mentioned above. \n",
        "\n",
        "From Sprint 2 to Spring 4, I was able to improve the accuracies of the models from 57% to 88%. I was able to acheive these gains mainly by working on the hybrid model of training models on both Binary and Multi classifaction. I think, the biggest boost was given by the numerous generated features which are very much relevant in identifying the similarities between Title and Text.\n",
        "\n",
        "### **What did not work?**\n",
        "\n",
        "Initially, I was trying to amalagamate, Liar-Liar dataset with Kaggle and Politifact datasets. As there was no dedicated title feature in the Liar-Liar dataset, I was using the topic as the title. But that didn't give me any good result. Later I started to explore to find the relevant dataset from Fakenews Challenge, which was apt for this feature as explained above. \n",
        "\n",
        "I was also only performing Binary classification at first with only a basic features such as Cosine Similarity between the TF-IDF vectors of title and text. But that gave me poor accuracies. Generating more features from different techniqes worked pretty well.\n",
        "\n",
        "I realized just working on individual cleaned words is not sufficient. So I worked on unigram bigram and trigram features. \n",
        "\n",
        "### **What alternatives did you try?**\n",
        "\n",
        "I tried to work on Liar-Liar Dataset,but later swithed to Fakenews challenge dataset.\n",
        "\n",
        "Tried generating multiple polynomial equations and chose the best one after running a few iterations.\n",
        "\n",
        "I started to perform LDA, but then switched to LSA as I wanted to try how it works, considering that we have already used LDA for other features.\n",
        "\n",
        "Tried to choose best features by using Gini Index. But then considering the number of features not many, I ended up using all the features by normalizing them using StandardScaler.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KELyb45uny3",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPbxJKXZrhgP",
        "colab_type": "code",
        "outputId": "6adbb9bc-0385-486a-b9e7-79b9972ad8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import gensim\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn import metrics\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WaBgPVmu0Su",
        "colab_type": "text"
      },
      "source": [
        "# Load the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMRGX7Nxu4vA",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle Fake News Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An07unApxzX_",
        "colab_type": "code",
        "outputId": "4a802d16-6de1-4dd2-939d-e3de5362b27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y6nxTktrkTx",
        "colab_type": "code",
        "outputId": "40f74dbb-2529-4f17-b9dd-0632386136f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fake_train = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/train_fakenews.csv\",sep=',')\n",
        "fake_test = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/test_fakenews.csv\",sep=',')\n",
        "fake_submit = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/submit_fakenews.csv\",sep=',')\n",
        "\n",
        "\n",
        "fake_test = pd.merge(fake_test, fake_submit, on=\"id\", how='left')\n",
        "fake_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6-lJxNs7bk",
        "colab_type": "code",
        "outputId": "39ea7315-1c49-4807-ee57-f11066caa003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "fake_train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20800 entries, 0 to 20799\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      20800 non-null  int64 \n",
            " 1   title   20242 non-null  object\n",
            " 2   author  18843 non-null  object\n",
            " 3   text    20761 non-null  object\n",
            " 4   label   20800 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 812.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RhUAdHis9Zo",
        "colab_type": "code",
        "outputId": "390a0703-c933-4499-c772-182f867e4c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fake_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20800</td>\n",
              "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
              "      <td>David Streitfeld</td>\n",
              "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20801</td>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20802</td>\n",
              "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
              "      <td>Common Dreams</td>\n",
              "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20803</td>\n",
              "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
              "      <td>Daniel Victor</td>\n",
              "      <td>If at first you don’t succeed, try a different...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20804</td>\n",
              "      <td>Keiser Report: Meme Wars (E995)</td>\n",
              "      <td>Truth Broadcast Network</td>\n",
              "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... label\n",
              "0  20800  ...     0\n",
              "1  20801  ...     1\n",
              "2  20802  ...     0\n",
              "3  20803  ...     1\n",
              "4  20804  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ3-lp0As_gG",
        "colab_type": "code",
        "outputId": "7812ed4b-5ef7-4ae1-fbb5-c1d46a84bb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "fake_test.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5200 entries, 0 to 5199\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      5200 non-null   int64 \n",
            " 1   title   5078 non-null   object\n",
            " 2   author  4697 non-null   object\n",
            " 3   text    5193 non-null   object\n",
            " 4   label   5200 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 243.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3rIZd2kviKj",
        "colab_type": "text"
      },
      "source": [
        "## Liar Liar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVb6lT2ZvlIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Liar-Liar dataset\n",
        "columns = [\n",
        "  'jsonid', \n",
        "  'label', \n",
        "  'text', \n",
        "  'subject', \n",
        "  'speaker', \n",
        "  'speakerjobtitle', \n",
        "  'stateinfo',\n",
        "  'partyaffiliation', \n",
        "  'barelytruecounts', \n",
        "  'falsecounts',\n",
        "  'halftruecounts',\n",
        "  'mostlytrueocunts',\n",
        "  'pantsonfirecounts',\n",
        "  'context'\n",
        "  ]\n",
        "\n",
        "train_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/train.tsv\",sep='\\t', names=columns)\n",
        "test_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/test.tsv\",sep='\\t', names=columns)\n",
        "valid_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/valid.tsv\",sep='\\t', names=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH1SCxC0vrI9",
        "colab_type": "code",
        "outputId": "0be04067-5f39-46fd-95ea-0184dfcbc87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "train_news.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jsonid</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakerjobtitle</th>\n",
              "      <th>stateinfo</th>\n",
              "      <th>partyaffiliation</th>\n",
              "      <th>barelytruecounts</th>\n",
              "      <th>falsecounts</th>\n",
              "      <th>halftruecounts</th>\n",
              "      <th>mostlytrueocunts</th>\n",
              "      <th>pantsonfirecounts</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>324.json</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1123.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9028.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jsonid        label  ... pantsonfirecounts              context\n",
              "0   2635.json        false  ...               0.0             a mailer\n",
              "1  10540.json    half-true  ...               0.0      a floor speech.\n",
              "2    324.json  mostly-true  ...               9.0               Denver\n",
              "3   1123.json        false  ...              44.0       a news release\n",
              "4   9028.json    half-true  ...               2.0  an interview on CNN\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwv_eE-CvtIA",
        "colab_type": "code",
        "outputId": "360b9c69-8d4a-420b-a1ff-ae216ffd7754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "train_news.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10240 entries, 0 to 10239\n",
            "Data columns (total 14 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   jsonid             10240 non-null  object \n",
            " 1   label              10240 non-null  object \n",
            " 2   text               10240 non-null  object \n",
            " 3   subject            10238 non-null  object \n",
            " 4   speaker            10238 non-null  object \n",
            " 5   speakerjobtitle    7343 non-null   object \n",
            " 6   stateinfo          8032 non-null   object \n",
            " 7   partyaffiliation   10238 non-null  object \n",
            " 8   barelytruecounts   10238 non-null  float64\n",
            " 9   falsecounts        10238 non-null  float64\n",
            " 10  halftruecounts     10238 non-null  float64\n",
            " 11  mostlytrueocunts   10238 non-null  float64\n",
            " 12  pantsonfirecounts  10238 non-null  float64\n",
            " 13  context            10138 non-null  object \n",
            "dtypes: float64(5), object(9)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YENSdMZjwFlE",
        "colab_type": "text"
      },
      "source": [
        "## Politifact news dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oksu_sRQwI8w",
        "colab_type": "code",
        "outputId": "895a7e9f-3bb6-44d3-b60c-1fd6ac7c43a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "politifact_fake = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/politifact_dataset/Fake.csv\",sep=',')\n",
        "politifact_fake['label'] = 1\n",
        "politifact_true = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/politifact_dataset/True.csv\",sep=',')\n",
        "politifact_true['label'] = 0\n",
        "df_politifact = pd.concat([politifact_fake, politifact_true])\n",
        "df_politifact.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...     1\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...     1\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...     1\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...     1\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MinmO8AwLAv",
        "colab_type": "code",
        "outputId": "7a89d3eb-9d6e-4c8f-a0c8-9b0541448998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_politifact.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 44898 entries, 0 to 21416\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    44898 non-null  object\n",
            " 1   text     44898 non-null  object\n",
            " 2   subject  44898 non-null  object\n",
            " 3   date     44898 non-null  object\n",
            " 4   label    44898 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 2.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brmQejKeXe2F",
        "colab_type": "text"
      },
      "source": [
        "## Stance Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MN4hyKUXfPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bodies = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/train_bodies.csv\")\n",
        "train_stances = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/train_stances.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImHQiTGmgnBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bodies = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/competition_test_bodies.csv\")\n",
        "test_stances = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/competition_test_stances.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxAGgnfrXmjN",
        "colab_type": "code",
        "outputId": "27efaf11-7115-4bd0-ad60-3674d8be56b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train_stances.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49972 entries, 0 to 49971\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Headline  49972 non-null  object\n",
            " 1   Body ID   49972 non-null  int64 \n",
            " 2   Stance    49972 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSPzF9bRXqmI",
        "colab_type": "code",
        "outputId": "e1748090-0577-467d-f021-cbb741610766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "train_bodies.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1683 entries, 0 to 1682\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Body ID      1683 non-null   int64 \n",
            " 1   articleBody  1683 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 26.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb0w_jg4gwXq",
        "colab_type": "code",
        "outputId": "450c04e2-4348-4414-a4c0-153777e61cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "test_stances.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25413 entries, 0 to 25412\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Headline  25413 non-null  object\n",
            " 1   Body ID   25413 non-null  int64 \n",
            " 2   Stance    25413 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 595.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWb53OJZTBj",
        "colab_type": "code",
        "outputId": "4e300f6f-d32c-4842-ce48-cf032abe1159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_bodies.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>Posting photos of a gun-toting child online, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        0  A small meteorite crashed into a wooded area i...\n",
              "1        4  Last week we hinted at what was to come as Ebo...\n",
              "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
              "3        6  Posting photos of a gun-toting child online, I...\n",
              "4        7  At least 25 suspected Boko Haram insurgents we..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42scoYaPZoka",
        "colab_type": "code",
        "outputId": "b7d5940b-ec63-4d93-ba2c-35ba82f39611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_stances.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  Body ID     Stance\n",
              "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
              "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
              "4  Spider burrowed through tourist's stomach and ...     1923   disagree"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBADndU1cIH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_train_stances = pd.merge(left=train_stances, right=train_bodies, how='left', left_on='Body ID', right_on='Body ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpm_JKOKey3X",
        "colab_type": "code",
        "outputId": "da663f94-15d3-4678-e357-b11c761bf713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_train_stances.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Police find mass graves with at least '15 bodi...  ...  Danny Boyle is directing the untitled film\\n\\n...\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...  ...  Hundreds of Palestinians were evacuated from t...\n",
              "2  Christian Bale passes on role of Steve Jobs, a...  ...  30-year-old Moscow resident was hospitalized w...\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...  ...  (Reuters) - A Canadian soldier was shot at the...\n",
              "4  Spider burrowed through tourist's stomach and ...  ...  Fear not arachnophobes, the story of Bunbury's...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpP_i-wHe3qG",
        "colab_type": "code",
        "outputId": "8d7722d9-2b9b-4d94-d4e5-f05eaf59ec4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_train_stances.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKs5bV9eg8he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_test_stances = pd.merge(left=test_stances, right=test_bodies, how='left', left_on='Body ID', right_on='Body ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tscskpNhEIT",
        "colab_type": "code",
        "outputId": "47376514-470d-4c0e-91e0-1df873691663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_test_stances.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
              "      <td>2008</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A RESPECTED senior French police officer inves...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
              "      <td>1550</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Dave Morin's social networking company Path is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
              "      <td>2</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
              "      <td>1793</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
              "      <td>37</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>An airline passenger headed to Dallas was remo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Ferguson riots: Pregnant woman loses eye after...  ...  A RESPECTED senior French police officer inves...\n",
              "1  Crazy Conservatives Are Sure a Gitmo Detainee ...  ...  Dave Morin's social networking company Path is...\n",
              "2  A Russian Guy Says His Justin Bieber Ringtone ...  ...  A bereaved Afghan mother took revenge on the T...\n",
              "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...  ...  Hewlett-Packard is officially splitting in two...\n",
              "4  Argentina's President Adopts Boy to End Werewo...  ...  An airline passenger headed to Dallas was remo...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOMN6GwhHwi",
        "colab_type": "code",
        "outputId": "0318cd29-fcb5-46fc-dec9-a683a18a3500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_test_stances.isnull().sum()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1fUqGc1jVGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_stance = pd.concat([final_train_stances, final_test_stances], axis=0, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3didRiuDmn3i",
        "colab_type": "code",
        "outputId": "61b29c56-dc84-46e7-ef12-c8c3f04b9f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Police find mass graves with at least '15 bodi...  ...  Danny Boyle is directing the untitled film\\n\\n...\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...  ...  Hundreds of Palestinians were evacuated from t...\n",
              "2  Christian Bale passes on role of Steve Jobs, a...  ...  30-year-old Moscow resident was hospitalized w...\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...  ...  (Reuters) - A Canadian soldier was shot at the...\n",
              "4  Spider burrowed through tourist's stomach and ...  ...  Fear not arachnophobes, the story of Bunbury's...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgtlkv4Bjmlb",
        "colab_type": "code",
        "outputId": "e9229be8-db3f-4f35-f9fd-631129663635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_stance.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VoMjT5r7dS",
        "colab_type": "code",
        "outputId": "dafcc272-44ca-443f-e05d-3ff5369493a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "final_stance.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75385, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOnxj2x7lBCb",
        "colab_type": "code",
        "outputId": "6583d26b-8ef8-4e1c-9e99-7e38e9a846eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_stance.drop('Body ID', axis=1, inplace=True)\n",
        "final_stance.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75385 entries, 0 to 75384\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Headline     75385 non-null  object\n",
            " 1   Stance       75385 non-null  object\n",
            " 2   articleBody  75385 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN9Z-T1Pjz05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_stance.rename(columns={'Headline': 'title', 'articleBody': 'text', 'Stance': 'label'}, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjwen7ohlT3-",
        "colab_type": "code",
        "outputId": "47d021fc-59d0-497a-abfa-6a01120a364c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_stance.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75385 entries, 0 to 75384\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   75385 non-null  object\n",
            " 1   label   75385 non-null  object\n",
            " 2   text    75385 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvP_7GUqv0k5",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPYFQRTYv3-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbvDMp_cdACQ",
        "colab_type": "text"
      },
      "source": [
        "# Feature 11: Title Vs Body \n",
        "**Name: Monica Dommaraju**\n",
        "\n",
        "\n",
        "My main idea of Title vs Body factor analysis is to pick Title and text features from each of these datasets and try to extract as many distilled features and then run models for classsification.\n",
        "\n",
        "1. Kaggle Fake News Dataset\n",
        "2. Liar-Liar Dataset\n",
        "3. politifact news Dataset\n",
        "4. Stance Dataset (from FakeNews Detection Challenge)\n",
        "\n",
        "\n",
        "`kaggle Fake News Dataset` Dataset and `Politifact news dataset` contains the title and text columns, but Liar-Liar Dataset contains only text column with out title text (I beleive `subject` feature in the Liar-Liar Dataset is not same the title). This made look for alternative datasets, so I can work on multi label classification. \n",
        "\n",
        "  I found Stance Dataset posted by FakeNews Detection\n",
        "\n",
        "\n",
        "and perform the following steps\n",
        "* Amalgamate all three datasets\n",
        "* cleaning the dataset \n",
        "* visualizing it using wordcloud\n",
        "* Calculate cosine similarity between Tile and Body for each sample\n",
        "* Calculate Number of matching strong words between these two columns\n",
        "* Run multiple classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD3gFuyAfhfL",
        "colab_type": "text"
      },
      "source": [
        "## Amalgamation\n",
        "**Amalgamate Kaggle Fake News Dataset and Politifact News Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnLv9GDq6lII",
        "colab_type": "code",
        "outputId": "37d75b1d-274e-4c03-d6b2-0af73d6af5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_kaggle = pd.concat([fake_train, fake_test])\n",
        "df_kaggle.shape\n",
        "# Pick only title and text columns from Kaggle dataset\n",
        "df_final = df_kaggle[['title', 'text', 'label']]\n",
        "df_final.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63wgvvKW8DUQ",
        "colab_type": "code",
        "outputId": "0d688776-0ca3-4535-ed49-085b25129cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_final = df_final.append(df_politifact[['title', 'text', 'label']])\n",
        "\n",
        "# Todo: Remove this sample technique when running the model finally\n",
        "# Running it with 25% of sample due to hardware constraints\n",
        "df_final = df_final.sample(frac=0.01).reset_index(drop=True)\n",
        "print (df_final.shape)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...     0\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...     1\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...     1\n",
              "3                                                NaN  ...     1\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4hh7XKO0VDg",
        "colab_type": "code",
        "outputId": "ccb53cb3-ef55-47bb-b75e-9b4890da1b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Todo: Remove this sample technique when running the model finally\n",
        "# Running it with 25% of sample due to hardware constraint\n",
        "\n",
        "final_stance = final_stance.sample(frac=0.01).reset_index(drop=True)\n",
        "print(final_stance.shape)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                               text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...  The World Health Organisation is investigating...\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...  ISIS released a video purportedly showing the ...\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...  After being officially dead for 48 minutes and...\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...  HBO's subscription streaming service will be c...\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...  By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl7YNe_j2duz",
        "colab_type": "text"
      },
      "source": [
        "## Clean the title and text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJPMlwWI2cdd",
        "colab_type": "code",
        "outputId": "339919b1-cf9a-433a-9e37-34eacacb74bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Cleaning on Fake News Dataset\n",
        "final_stance['cleaned_title'] = final_stance[\"title\"].map(lambda x: cleaning(x))\n",
        "final_stance['cleaned_text'] = final_stance[\"text\"].map(lambda x: cleaning(x))\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                       cleaned_text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...  world health organis investig report isi milit...\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...  isi releas video purport show execut british a...\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...  offici dead minut heart restart year old massa...\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...  hbo subscript stream servic call hbo expect co...\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...  sylvia westal beirut reuter iraqi pilot join i...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09dhiP1kLPuP",
        "outputId": "143aa36a-0b8c-4d7a-987d-1325452b5a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Cleaning on Amalamated Kaggle and Fake News dataset.\n",
        "print (final_stance.info())\n",
        "print (df_final.info())\n",
        "df_final['title'] = df_final['title'].astype('str')\n",
        "df_final['text'] = df_final['text'].astype('str')\n",
        "df_final['cleaned_title'] = df_final['title'].astype('str').map(lambda x: cleaning(x))\n",
        "df_final['cleaned_text'] = df_final['text'].astype('str').map(lambda x: cleaning(x))\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 754 entries, 0 to 753\n",
            "Data columns (total 5 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   title          754 non-null    object\n",
            " 1   label          754 non-null    object\n",
            " 2   text           754 non-null    object\n",
            " 3   cleaned_title  754 non-null    object\n",
            " 4   cleaned_text   754 non-null    object\n",
            "dtypes: object(5)\n",
            "memory usage: 29.6+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 709 entries, 0 to 708\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   700 non-null    object\n",
            " 1   text    708 non-null    object\n",
            " 2   label   709 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 16.7+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                       cleaned_text\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...  london reuter one chennai six group ex british...\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...  iran fulfil oblig nuclear deal reach obama adm...\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...  nanci pelosi leader hous democrat long time we...\n",
              "3                                                nan  ...  watch video fulli document heavi connect islam...\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...  share facebook result fatigu dizzi short breat...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YT0K144u4Sn",
        "colab_type": "text"
      },
      "source": [
        "##Count of sentences in Title and Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i23-04X1vCEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def count_sentences(data):\n",
        "  data['count_title_sentences'] = data['title'].apply(lambda x: len(sent_tokenize(x)))\n",
        "  data['count_text_sentences'] = data['text'].apply(lambda x: len(sent_tokenize(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih6qUaxKwKEs",
        "colab_type": "code",
        "outputId": "7b769725-4b00-4b5d-b607-5e4e2d37b801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Calculate the total number of sentences in Title and Text on Multiclass labeled Dataset\n",
        "count_sentences(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_text_sentences\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                   18\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                    1\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                    9\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                   16\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                   18\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7jriusj0LwS3",
        "outputId": "5dcff0d7-8409-43fa-bf92-37168cc6a3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Calculate the total number of sentences in Title and Text on Binary class labeled Dataset\n",
        "count_sentences(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_text_sentences\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...                   13\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...                   20\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...                   17\n",
              "3                                                nan  ...                    5\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...                   50\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny7a9yBAjUH-",
        "colab_type": "text"
      },
      "source": [
        "## Generating N-Grams and Their Lengths\n",
        "\n",
        "Uni, Bi and Tri grams are generated so we can extract some count fearures for each of these grams along with the count features of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ02lCv9zGLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def ngram(text, n):\n",
        "    n_grams = ngrams(word_tokenize(text), n)\n",
        "    return [ '_'.join(grams) for grams in n_grams]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiLDipsLf_VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_ngram_features(data):\n",
        "  data[\"title_unigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 1))\n",
        "  data[\"text_unigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 1))\n",
        "  data[\"count_title_unigram\"] = list(data.apply(lambda x: len(x['title_unigram']), axis=1))\n",
        "  data[\"count_text_unigram\"] = list(data.apply(lambda x: len(x['text_unigram']), axis=1))\n",
        "  data[\"unique_count_title_unigram\"] = list(data.apply(lambda x: len(set(x['title_unigram'])), axis=1))\n",
        "  data[\"unique_count_text_unigram\"] = list(data.apply(lambda x: len(set(x['text_unigram'])), axis=1))\n",
        "\n",
        "  data[\"title_bigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 2))\n",
        "  data[\"text_bigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 2))\n",
        "  data[\"count_title_bigram\"] = list(data.apply(lambda x: len(x['title_bigram']), axis=1))\n",
        "  data[\"count_text_bigram\"] = list(data.apply(lambda x: len(x['text_bigram']), axis=1))\n",
        "  data[\"unique_count_title_bigram\"] = list(data.apply(lambda x: len(set(x['title_bigram'])), axis=1))\n",
        "  data[\"unique_count_text_bigram\"] = list(data.apply(lambda x: len(set(x['text_bigram'])), axis=1))\n",
        "\n",
        "  data[\"title_trigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 3))\n",
        "  data[\"text_trigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 3))\n",
        "  data[\"count_title_trigram\"] = list(data.apply(lambda x: len(x['title_trigram']), axis=1))\n",
        "  data[\"count_text_trigram\"] = list(data.apply(lambda x: len(x['text_trigram']), axis=1))\n",
        "  data[\"unique_count_title_trigram\"] = list(data.apply(lambda x: len(set(x['title_trigram'])), axis=1))\n",
        "  data[\"unique_count_text_trigram\"] = list(data.apply(lambda x: len(set(x['text_trigram'])), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIMY2VMOzoGf",
        "colab_type": "code",
        "outputId": "2ebb7707-cdaf-4e93-d33e-7a48c8ce5895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "# Generate n-grams of title and text. Then calculate the length of unigram, bigram and trigrams on Multiclass labeled DS\n",
        "generate_ngram_features(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... unique_count_text_trigram\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                       240\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                         9\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                        76\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                       223\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                       315\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jGaaIKESMAjJ",
        "outputId": "a8d0503f-e3cf-47bb-a51f-2563a13db0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Generate n-grams of title and text. Then calculate the length of unigram, bigram and trigrams on Binary class labeled DS\n",
        "generate_ngram_features(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... unique_count_text_trigram\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...                       246\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...                       418\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...                       222\n",
              "3                                                nan  ...                        28\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...                       432\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMbs960ppppn",
        "colab_type": "text"
      },
      "source": [
        "## Common N-grams between Title and Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTd7_gfWqhiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def common_ngrams_in_text(data):\n",
        "  data[\"count_title_unigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_unigram'] if w in set(x['text_unigram'])]), axis=1))\n",
        "  data[\"count_title_bigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_bigram'] if w in set(x['text_bigram'])]), axis=1))\n",
        "  data[\"count_title_trigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_trigram'] if w in set(x['text_trigram'])]), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAYK6twTzoU7",
        "colab_type": "code",
        "outputId": "b3185954-b028-421e-9d7c-1d09a3b8eebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Calculate the total number of common title n-grams in the text on Multi class labeled DS\n",
        "common_ngrams_in_text(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_title_trigrams_in_text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                          0.0\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                          0.0\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                          0.0\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                          0.0\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                          0.0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bR6p2anMZHh",
        "outputId": "379da279-d0a6-4f09-fe07-f67eb0844cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# Calculate the total number of common title n-grams in the text on Binary class labeled DS\n",
        "common_ngrams_in_text(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_title_trigrams_in_text\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...                          0.0\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...                          0.0\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...                          0.0\n",
              "3                                                nan  ...                          0.0\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...                          0.0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fdnUxbcc8Ky",
        "colab_type": "text"
      },
      "source": [
        "## TF_IDF and Cosine Similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ_095uI6jSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat_title_text(data):\n",
        "  data['cleaned_title_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkCrgPHm7Z2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf(data):\n",
        "  concat_title_text(data)\n",
        "  combined_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True)\n",
        "  combined_vectors.fit(data[\"cleaned_title_text\"])\n",
        "  combined_vectors_dictionary = combined_vectors.vocabulary_\n",
        "  # print (combined_vectors_dictionary)\n",
        "\n",
        "  # Now lets generate the TfIdf Vectors seperately for Title and Text from the vector dictionary\n",
        "  # built above\n",
        "  title_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True, vocabulary=combined_vectors_dictionary)\n",
        "  title_tfidf_vectors = title_vectors.fit_transform(data['cleaned_title'])\n",
        "  print (title_tfidf_vectors.shape)\n",
        "  # print (\"\\n Title TF-IDF Vectors\")\n",
        "  # print (title_vectors.get_feature_names())     \n",
        "\n",
        "  text_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True, vocabulary=combined_vectors_dictionary)\n",
        "  text_tfidf_vectors = text_vectors.fit_transform(data['cleaned_text'])\n",
        "\n",
        "  # print (\"\\n Text TF-IDF Vectors\")\n",
        "  # print (text_vectors.get_feature_names())\n",
        "  print (text_tfidf_vectors.shape)\n",
        "\n",
        "  return title_tfidf_vectors, text_tfidf_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox19gteECU_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similarity_score(data, title_vectors, text_vectors):\n",
        "  similarity_score = []\n",
        "  for i in range(len(data)):\n",
        "      similarity_score.append(1 - cosine(title_vectors[i], text_vectors[i]))\n",
        "  return similarity_score\n",
        "\n",
        "def tf_idf_similarity_score(data):\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf(data)\n",
        "  data['similarity_title_text'] = similarity_score(data, title_tfidf_vectors.toarray(), text_tfidf_vectors.toarray())\n",
        "  return title_tfidf_vectors, text_tfidf_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ysINkJ9_bQ",
        "colab_type": "code",
        "outputId": "18dd9cc6-b739-4940-bd5a-659bbbbdabab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "# Tf-IDF and Cosine Similarities on Multiclass labeled dataset\n",
        "stance_title_tfidf_vectors, stance_text_tfidf_vectors = tf_idf_similarity_score(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 61079)\n",
            "(754, 61079)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ebola infect isi milit mosul world health orga...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>meteor leaf foot crater near managua airport i...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>mexico prosecutor student st mass graf offici ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>letter lego parent make import point gender hb...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... similarity_title_text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                   NaN\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                   NaN\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                   NaN\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                   0.0\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                   NaN\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C39ZDz6dMoHO",
        "outputId": "58f01428-8bf4-4640-e0fd-ac0c548a9dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Tf-IDF and Cosine Similarities on Binary class labeled dataset\n",
        "\n",
        "df_title_tfidf_vectors, df_text_tfidf_vectors = tf_idf_similarity_score(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709, 164516)\n",
            "(709, 164516)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>0.134715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>0.019066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nan watch video fulli document heavi connect i...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>0.224354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... similarity_title_text\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...              0.134715\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...              0.019066\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...              0.000000\n",
              "3                                                nan  ...                   NaN\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...              0.224354\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213tPwgLUw1p",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec using Google Corpus and Similarity scores\n",
        "\n",
        "Now we will generate the word vectors for title and text by training each of the unigram words on Google News corpus. This is used to find synonyms for a word, which will then be used to calculate the similaries scores between the synonyms rather than the exact words.\n",
        "\n",
        "To use Word2Vec, we need title and text with out lemmatization and stop words performed on it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgNHgN0RVgsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_simple_tokenize(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    return np.array(news_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5LHCL0iXoeq",
        "colab_type": "code",
        "outputId": "774a12cf-09aa-46cc-842b-5fe37b1dffca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "google_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/Shared drives/SheCodes/Datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srzLEcBepWny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK_i7l0_AwSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from functools import reduce\n",
        "\n",
        "def word2vec_similarity_score(data):\n",
        "  data['word2vec_cleaned_title'] = data[\"title\"].map(lambda x: cleaning_simple_tokenize(x))\n",
        "  data['word2vec_cleaned_text'] = data[\"text\"].map(lambda x: cleaning_simple_tokenize(x))\n",
        "\n",
        "  title_word2vec = data['word2vec_cleaned_title'].map(lambda words: reduce(np.add, [google_model[word] for word in words if word in google_model], [0.]*300))\n",
        "  text_word2vec = data['word2vec_cleaned_text'].map(lambda words: reduce(np.add, [google_model[word] for word in words if word in google_model], [0.]*300))\n",
        "\n",
        "  data['word2vec_similarity_title_text'] = similarity_score(data, np.array(title_word2vec), np.array(text_word2vec))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRT6nREKrcYR",
        "colab_type": "code",
        "outputId": "8c6591d6-60a8-4eb5-eb1a-4a0840a982e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "# Word2Vec and Cosine Similarities on Multiclass labeled dataset\n",
        "\n",
        "word2vec_similarity_score(final_stance)\n",
        "final_stance.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ebola infect isi milit mosul world health orga...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[has, ebola, infected, isis, militants, in, mo...</td>\n",
              "      <td>[the, world, health, organisation, is, investi...</td>\n",
              "      <td>0.609588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>meteor leaf foot crater near managua airport i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[meteor, leaves, foot, crater, near, managua, ...</td>\n",
              "      <td>[isis, released, a, video, purportedly, showin...</td>\n",
              "      <td>0.249012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>mexico prosecutor student st mass graf offici ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[mexico, prosecutor, students, not, in, st, ma...</td>\n",
              "      <td>[after, being, officially, dead, for, minutes,...</td>\n",
              "      <td>0.528046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>letter lego parent make import point gender hb...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[letter, from, lego, to, parents, in, the, s, ...</td>\n",
              "      <td>[hbo, s, subscription, streaming, service, wil...</td>\n",
              "      <td>0.691017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[islamic, state, militants, have, been, flying...</td>\n",
              "      <td>[by, sylvia, westall, beirut, reuters, iraqi, ...</td>\n",
              "      <td>0.780305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... word2vec_similarity_title_text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                       0.609588\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                       0.249012\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                       0.528046\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                       0.691017\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                       0.780305\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tk7QoTQwNx3F",
        "outputId": "678f9c38-acba-43d2-d748-8ed0d8050213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Word2Vec and Cosine Similarities on Binary Class labeled dataset\n",
        "\n",
        "word2vec_similarity_score(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>0.134715</td>\n",
              "      <td>[indian, tycoon, s, extradition, hearing, told...</td>\n",
              "      <td>[london, reuters, one, of, the, chennai, six, ...</td>\n",
              "      <td>0.649405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>[iran, proves, republicans, are, full, of, cra...</td>\n",
              "      <td>[iran, is, fulfilling, their, obligations, und...</td>\n",
              "      <td>0.806206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[breaking, dems, keep, progressive, bent, in, ...</td>\n",
              "      <td>[nancy, pelosi, has, been, the, leader, of, th...</td>\n",
              "      <td>0.710030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nan watch video fulli document heavi connect i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, this, video, fully, documented, the, h...</td>\n",
              "      <td>0.187610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>0.224354</td>\n",
              "      <td>[centrum, the, world, s, most, popular, multiv...</td>\n",
              "      <td>[share, on, facebook, this, can, result, in, f...</td>\n",
              "      <td>0.783566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... word2vec_similarity_title_text\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...                       0.649405\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...                       0.806206\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...                       0.710030\n",
              "3                                                nan  ...                       0.187610\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...                       0.783566\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05VwWbtpGR44",
        "colab_type": "text"
      },
      "source": [
        "## LSA Topic Modelling using SVD and Cosine Similarities\n",
        "\n",
        "Lets now try to find topics from the title and text using SVD. Then find the cosine similarity between the generated title and text topics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxO043YTGQ7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "def svd(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  truncated_svd = TruncatedSVD(n_components=100, n_iter=10)\n",
        "  combined_vectors = vstack([title_tfidf_vectors, text_tfidf_vectors])\n",
        "  truncated_svd.fit(combined_vectors)\n",
        "\n",
        "  title_svd = truncated_svd.transform(title_tfidf_vectors)\n",
        "  print (title_svd.shape)\n",
        "  text_svd = truncated_svd.transform(text_tfidf_vectors)\n",
        "  print (text_svd.shape)\n",
        "  return title_svd, text_svd\n",
        "\n",
        "def topic_similarity_score(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  title_svd_vectors, text_svd_vectors = svd(data, title_tfidf_vectors, text_tfidf_vectors)\n",
        "  data['topics_similarity_title_text'] = similarity_score(data, title_svd_vectors, text_svd_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPszGIt5HqnW",
        "colab_type": "code",
        "outputId": "6a73645d-bbd1-45a6-8da7-c5d60eb85fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "# LSA Topic Modelling and Cosine Similarities on Multi Class labeled dataset\n",
        "topic_similarity_score(final_stance, stance_title_tfidf_vectors, stance_text_tfidf_vectors)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 100)\n",
            "(754, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ebola infect isi milit mosul world health orga...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[has, ebola, infected, isis, militants, in, mo...</td>\n",
              "      <td>[the, world, health, organisation, is, investi...</td>\n",
              "      <td>0.609588</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>meteor leaf foot crater near managua airport i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[meteor, leaves, foot, crater, near, managua, ...</td>\n",
              "      <td>[isis, released, a, video, purportedly, showin...</td>\n",
              "      <td>0.249012</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>mexico prosecutor student st mass graf offici ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[mexico, prosecutor, students, not, in, st, ma...</td>\n",
              "      <td>[after, being, officially, dead, for, minutes,...</td>\n",
              "      <td>0.528046</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>letter lego parent make import point gender hb...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[letter, from, lego, to, parents, in, the, s, ...</td>\n",
              "      <td>[hbo, s, subscription, streaming, service, wil...</td>\n",
              "      <td>0.691017</td>\n",
              "      <td>-0.028062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[islamic, state, militants, have, been, flying...</td>\n",
              "      <td>[by, sylvia, westall, beirut, reuters, iraqi, ...</td>\n",
              "      <td>0.780305</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topics_similarity_title_text\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...                          NaN\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...                          NaN\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...                          NaN\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...                    -0.028062\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...                          NaN\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yoLwfmnJON-w",
        "outputId": "fd623492-6325-41c4-e397-1f37de042371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# LSA Topic Modelling and Cosine Similarities on Binary Class labeled dataset\n",
        "\n",
        "topic_similarity_score(df_final, df_title_tfidf_vectors, df_text_tfidf_vectors)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709, 100)\n",
            "(709, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>0.134715</td>\n",
              "      <td>[indian, tycoon, s, extradition, hearing, told...</td>\n",
              "      <td>[london, reuters, one, of, the, chennai, six, ...</td>\n",
              "      <td>0.649405</td>\n",
              "      <td>0.990605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>[iran, proves, republicans, are, full, of, cra...</td>\n",
              "      <td>[iran, is, fulfilling, their, obligations, und...</td>\n",
              "      <td>0.806206</td>\n",
              "      <td>0.271865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[breaking, dems, keep, progressive, bent, in, ...</td>\n",
              "      <td>[nancy, pelosi, has, been, the, leader, of, th...</td>\n",
              "      <td>0.710030</td>\n",
              "      <td>0.187350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nan watch video fulli document heavi connect i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, this, video, fully, documented, the, h...</td>\n",
              "      <td>0.187610</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>0.224354</td>\n",
              "      <td>[centrum, the, world, s, most, popular, multiv...</td>\n",
              "      <td>[share, on, facebook, this, can, result, in, f...</td>\n",
              "      <td>0.783566</td>\n",
              "      <td>0.999797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topics_similarity_title_text\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...                     0.990605\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...                     0.271865\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...                     0.187350\n",
              "3                                                nan  ...                          NaN\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...                     0.999797\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNDRqdAxvQIZ",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis and Cosine Similaties\n",
        "\n",
        "Now lets assign polarity sentiment score to the title and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cP2_j1yvoIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def compute_sentiment(sentences):\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        vs = sentiment_analyzer.polarity_scores(sentence)\n",
        "        result.append(vs)\n",
        "    return pd.DataFrame(result).mean()\n",
        "\n",
        "def title_sentiment(data):\n",
        "  data['title_sentences'] = data['title'].apply(lambda x: sent_tokenize(x))\n",
        "  data = pd.concat([data, data['title_sentences'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
        "  data.rename(columns={'compound':'title_compound', 'neg':'title_neg', 'neu':'title_neu', 'pos':'title_pos'}, inplace=True)\n",
        "  return data\n",
        "\n",
        "\n",
        "def text_sentiment(data):\n",
        "  data['text_sentences'] = data['text'].apply(lambda x: sent_tokenize(x))\n",
        "  data = pd.concat([data, data['text_sentences'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
        "  data.rename(columns={'compound':'text_compound', 'neg':'text_neg', 'neu':'text_neu', 'pos':'text_pos'}, inplace=True)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAgdY_Dkwx79",
        "colab_type": "code",
        "outputId": "9a45bb1c-ce9d-44a9-a21d-f00cccd13b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Sentiment Analysis on Multi Class labeled dataset\n",
        "\n",
        "final_stance = title_sentiment(final_stance)\n",
        "final_stance = text_sentiment(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>title_sentences</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_sentences</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Has Ebola infected Isis militants in Mosul?</td>\n",
              "      <td>discuss</td>\n",
              "      <td>The World Health Organisation is investigating...</td>\n",
              "      <td>ebola infect isi milit mosul</td>\n",
              "      <td>world health organis investig report isi milit...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[ebola, infect, isi, milit, mosul]</td>\n",
              "      <td>[world, health, organis, investig, report, isi...</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>[ebola_infect, infect_isi, isi_milit, milit_mo...</td>\n",
              "      <td>[world_health, health_organis, organis_investi...</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>[ebola_infect_isi, infect_isi_milit, isi_milit...</td>\n",
              "      <td>[world_health_organis, health_organis_investig...</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ebola infect isi milit mosul world health orga...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[has, ebola, infected, isis, militants, in, mo...</td>\n",
              "      <td>[the, world, health, organisation, is, investi...</td>\n",
              "      <td>0.609588</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Has Ebola infected Isis militants in Mosul?]</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4939</td>\n",
              "      <td>[The World Health Organisation is investigatin...</td>\n",
              "      <td>0.087056</td>\n",
              "      <td>0.874944</td>\n",
              "      <td>0.038000</td>\n",
              "      <td>-0.100422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meteor Leaves 40-Foot Crater Near Managua's Ai...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ISIS released a video purportedly showing the ...</td>\n",
              "      <td>meteor leaf foot crater near managua airport</td>\n",
              "      <td>isi releas video purport show execut british a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[meteor, leaf, foot, crater, near, managua, ai...</td>\n",
              "      <td>[isi, releas, video, purport, show, execut, br...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>[meteor_leaf, leaf_foot, foot_crater, crater_n...</td>\n",
              "      <td>[isi_releas, releas_video, video_purport, purp...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>[meteor_leaf_foot, leaf_foot_crater, foot_crat...</td>\n",
              "      <td>[isi_releas_video, releas_video_purport, video...</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>meteor leaf foot crater near managua airport i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[meteor, leaves, foot, crater, near, managua, ...</td>\n",
              "      <td>[isis, released, a, video, purportedly, showin...</td>\n",
              "      <td>0.249012</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Meteor Leaves 40-Foot Crater Near Managua's A...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[ISIS released a video purportedly showing the...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico prosecutor: Students not in 1st mass gr...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>After being officially dead for 48 minutes and...</td>\n",
              "      <td>mexico prosecutor student st mass graf</td>\n",
              "      <td>offici dead minut heart restart year old massa...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>[mexico, prosecutor, student, st, mass, graf]</td>\n",
              "      <td>[offici, dead, minut, heart, restart, year, ol...</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>[mexico_prosecutor, prosecutor_student, studen...</td>\n",
              "      <td>[offici_dead, dead_minut, minut_heart, heart_r...</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>[mexico_prosecutor_student, prosecutor_student...</td>\n",
              "      <td>[offici_dead_minut, dead_minut_heart, minut_he...</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>mexico prosecutor student st mass graf offici ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[mexico, prosecutor, students, not, in, st, ma...</td>\n",
              "      <td>[after, being, officially, dead, for, minutes,...</td>\n",
              "      <td>0.528046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Mexico prosecutor: Students not in 1st mass g...</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.2960</td>\n",
              "      <td>[After being officially dead for 48 minutes an...</td>\n",
              "      <td>0.111222</td>\n",
              "      <td>0.792667</td>\n",
              "      <td>0.096111</td>\n",
              "      <td>-0.040711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Letter From Lego To Parents In The '70s Makes ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>HBO's subscription streaming service will be c...</td>\n",
              "      <td>letter lego parent make import point gender</td>\n",
              "      <td>hbo subscript stream servic call hbo expect co...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[letter, lego, parent, make, import, point, ge...</td>\n",
              "      <td>[hbo, subscript, stream, servic, call, hbo, ex...</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>[letter_lego, lego_parent, parent_make, make_i...</td>\n",
              "      <td>[hbo_subscript, subscript_stream, stream_servi...</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>[letter_lego_parent, lego_parent_make, parent_...</td>\n",
              "      <td>[hbo_subscript_stream, subscript_stream_servic...</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>letter lego parent make import point gender hb...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[letter, from, lego, to, parents, in, the, s, ...</td>\n",
              "      <td>[hbo, s, subscription, streaming, service, wil...</td>\n",
              "      <td>0.691017</td>\n",
              "      <td>-0.028062</td>\n",
              "      <td>[Letter From Lego To Parents In The '70s Makes...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.878</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>[HBO's subscription streaming service will be ...</td>\n",
              "      <td>0.019813</td>\n",
              "      <td>0.910812</td>\n",
              "      <td>0.069375</td>\n",
              "      <td>0.157981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Islamic State Militants Have Been Flying 3 Cap...</td>\n",
              "      <td>discuss</td>\n",
              "      <td>By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi ...</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>sylvia westal beirut reuter iraqi pilot join i...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[islam, state, milit, fli, captur, warplan, sy...</td>\n",
              "      <td>[sylvia, westal, beirut, reuter, iraqi, pilot,...</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>[islam_state, state_milit, milit_fli, fli_capt...</td>\n",
              "      <td>[sylvia_westal, westal_beirut, beirut_reuter, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>[islam_state_milit, state_milit_fli, milit_fli...</td>\n",
              "      <td>[sylvia_westal_beirut, westal_beirut_reuter, b...</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>islam state milit fli captur warplan syrian ai...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[islamic, state, militants, have, been, flying...</td>\n",
              "      <td>[by, sylvia, westall, beirut, reuters, iraqi, ...</td>\n",
              "      <td>0.780305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Islamic State Militants Have Been Flying 3 Ca...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[By Sylvia Westall\\n\\nBEIRUT (Reuters) – Iraqi...</td>\n",
              "      <td>0.041278</td>\n",
              "      <td>0.950944</td>\n",
              "      <td>0.007778</td>\n",
              "      <td>-0.145089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... text_compound\n",
              "0        Has Ebola infected Isis militants in Mosul?  ...     -0.100422\n",
              "1  Meteor Leaves 40-Foot Crater Near Managua's Ai...  ...      0.000000\n",
              "2  Mexico prosecutor: Students not in 1st mass gr...  ...     -0.040711\n",
              "3  Letter From Lego To Parents In The '70s Makes ...  ...      0.157981\n",
              "4  Islamic State Militants Have Been Flying 3 Cap...  ...     -0.145089\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WM1EE-vcPIIm",
        "outputId": "953bc2f7-d425-4c76-cc1d-cbd12dd833da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# Sentiment Analysis on Binary Class labeled dataset\n",
        "\n",
        "df_final = title_sentiment(df_final)\n",
        "df_final = text_sentiment(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>title_sentences</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_sentences</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian tycoon's extradition hearing told of ab...</td>\n",
              "      <td>LONDON (Reuters) - One of the  Chennai Six  gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>london reuter one chennai six group ex british...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[indian, tycoon, extradit, hear, told, abus, u...</td>\n",
              "      <td>[london, reuter, one, chennai, six, group, ex,...</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>[indian_tycoon, tycoon_extradit, extradit_hear...</td>\n",
              "      <td>[london_reuter, reuter_one, one_chennai, chenn...</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>[indian_tycoon_extradit, tycoon_extradit_hear,...</td>\n",
              "      <td>[london_reuter_one, reuter_one_chennai, one_ch...</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>indian tycoon extradit hear told abus uk chenn...</td>\n",
              "      <td>0.134715</td>\n",
              "      <td>[indian, tycoon, s, extradition, hearing, told...</td>\n",
              "      <td>[london, reuters, one, of, the, chennai, six, ...</td>\n",
              "      <td>0.649405</td>\n",
              "      <td>0.990605</td>\n",
              "      <td>[Indian tycoon's extradition hearing told of a...</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6369</td>\n",
              "      <td>[LONDON (Reuters) - One of the  Chennai Six  g...</td>\n",
              "      <td>0.091077</td>\n",
              "      <td>0.881462</td>\n",
              "      <td>0.027538</td>\n",
              "      <td>-0.248646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iran Proves Republicans Are Full Of Crap As U...</td>\n",
              "      <td>Iran is fulfilling their obligations under the...</td>\n",
              "      <td>1</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>iran fulfil oblig nuclear deal reach obama adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[iran, prof, republican, full, crap, unit, nat...</td>\n",
              "      <td>[iran, fulfil, oblig, nuclear, deal, reach, ob...</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>[iran_prof, prof_republican, republican_full, ...</td>\n",
              "      <td>[iran_fulfil, fulfil_oblig, oblig_nuclear, nuc...</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>[iran_prof_republican, prof_republican_full, r...</td>\n",
              "      <td>[iran_fulfil_oblig, fulfil_oblig_nuclear, obli...</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iran prof republican full crap unit nation say...</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>[iran, proves, republicans, are, full, of, cra...</td>\n",
              "      <td>[iran, is, fulfilling, their, obligations, und...</td>\n",
              "      <td>0.806206</td>\n",
              "      <td>0.271865</td>\n",
              "      <td>[ Iran Proves Republicans Are Full Of Crap As ...</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>[Iran is fulfilling their obligations under th...</td>\n",
              "      <td>0.056150</td>\n",
              "      <td>0.804200</td>\n",
              "      <td>0.139650</td>\n",
              "      <td>0.346365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Breaking: Dems Keep Progressive Bent In All-I...</td>\n",
              "      <td>Nancy Pelosi has been the leader of the House ...</td>\n",
              "      <td>1</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>nanci pelosi leader hous democrat long time we...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>[break, dem, keep, progress, bent, import, lea...</td>\n",
              "      <td>[nanci, pelosi, leader, hous, democrat, long, ...</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>[break_dem, dem_keep, keep_progress, progress_...</td>\n",
              "      <td>[nanci_pelosi, pelosi_leader, leader_hous, hou...</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>[break_dem_keep, dem_keep_progress, keep_progr...</td>\n",
              "      <td>[nanci_pelosi_leader, pelosi_leader_hous, lead...</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>break dem keep progress bent import leadership...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[breaking, dems, keep, progressive, bent, in, ...</td>\n",
              "      <td>[nancy, pelosi, has, been, the, leader, of, th...</td>\n",
              "      <td>0.710030</td>\n",
              "      <td>0.187350</td>\n",
              "      <td>[ Breaking: Dems Keep Progressive Bent In All-...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[Nancy Pelosi has been the leader of the House...</td>\n",
              "      <td>0.057059</td>\n",
              "      <td>0.823118</td>\n",
              "      <td>0.119824</td>\n",
              "      <td>0.199129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nan</td>\n",
              "      <td>Watch this video. Fully documented the heavy c...</td>\n",
              "      <td>1</td>\n",
              "      <td>nan</td>\n",
              "      <td>watch video fulli document heavi connect islam...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, video, fulli, document, heavi, connect...</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video, video_fulli, fulli_document, doc...</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>[]</td>\n",
              "      <td>[watch_video_fulli, video_fulli_document, full...</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nan watch video fulli document heavi connect i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>[watch, this, video, fully, documented, the, h...</td>\n",
              "      <td>0.187610</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[nan]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[Watch this video., Fully documented the heavy...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Centrum – The World's Most Popular Multivitami...</td>\n",
              "      <td>Share on Facebook This can result in fatigue, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>share facebook result fatigu dizzi short breat...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[centrum, world, popular, multivitamin, useles...</td>\n",
              "      <td>[share, facebook, result, fatigu, dizzi, short...</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>[centrum_world, world_popular, popular_multivi...</td>\n",
              "      <td>[share_facebook, facebook_result, result_fatig...</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>[centrum_world_popular, world_popular_multivit...</td>\n",
              "      <td>[share_facebook_result, facebook_result_fatigu...</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>centrum world popular multivitamin useless con...</td>\n",
              "      <td>0.224354</td>\n",
              "      <td>[centrum, the, world, s, most, popular, multiv...</td>\n",
              "      <td>[share, on, facebook, this, can, result, in, f...</td>\n",
              "      <td>0.783566</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>[Centrum – The World's Most Popular Multivitam...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.4754</td>\n",
              "      <td>[Share on Facebook This can result in fatigue,...</td>\n",
              "      <td>0.082300</td>\n",
              "      <td>0.804600</td>\n",
              "      <td>0.113120</td>\n",
              "      <td>0.058830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... text_compound\n",
              "0  Indian tycoon's extradition hearing told of ab...  ...     -0.248646\n",
              "1   Iran Proves Republicans Are Full Of Crap As U...  ...      0.346365\n",
              "2   Breaking: Dems Keep Progressive Bent In All-I...  ...      0.199129\n",
              "3                                                nan  ...      0.000000\n",
              "4  Centrum – The World's Most Popular Multivitami...  ...      0.058830\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0XDZJacHfvO",
        "colab_type": "text"
      },
      "source": [
        "## Features Distillation for Modelling\n",
        "\n",
        "Now, lets keep the features which are need for model training and remove all the raw title and text features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do4395gUIANt",
        "colab_type": "code",
        "outputId": "87e06deb-9b68-4bd9-c5e8-e77d9431aa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "distilled_features_cols = ['label', 'count_title_sentences', 'count_text_sentences', 'count_title_unigram', 'count_text_unigram',\n",
        "                'unique_count_title_unigram', 'unique_count_text_unigram', 'count_title_bigram', 'count_text_bigram',\n",
        "                'unique_count_title_bigram', 'unique_count_text_bigram', 'count_title_trigram', 'count_text_trigram',\n",
        "                'unique_count_title_trigram', 'unique_count_text_trigram', 'count_title_unigrams_in_text', 'count_title_bigrams_in_text',\n",
        "                'count_title_trigrams_in_text', 'similarity_title_text', 'topics_similarity_title_text', 'word2vec_similarity_title_text',\n",
        "                'title_neg', 'title_neu', 'title_pos', 'title_compound', 'text_neg', 'text_neu', 'text_pos', 'text_compound']\n",
        "\n",
        "df_final_multi = final_stance[distilled_features_cols]\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df_final_multi['label'])\n",
        "\n",
        "df_final_multi['label'] = le.transform(df_final_multi['label'])\n",
        "# print(le.inverse_transform(df_final_multi['label']))\n",
        "\n",
        "df_final_multi.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "      <td>160</td>\n",
              "      <td>4</td>\n",
              "      <td>259</td>\n",
              "      <td>4</td>\n",
              "      <td>231</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>3</td>\n",
              "      <td>240</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.609588</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4939</td>\n",
              "      <td>0.087056</td>\n",
              "      <td>0.874944</td>\n",
              "      <td>0.038000</td>\n",
              "      <td>-0.100422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.249012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>78</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>5</td>\n",
              "      <td>77</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.528046</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.2960</td>\n",
              "      <td>0.111222</td>\n",
              "      <td>0.792667</td>\n",
              "      <td>0.096111</td>\n",
              "      <td>-0.040711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>229</td>\n",
              "      <td>7</td>\n",
              "      <td>137</td>\n",
              "      <td>6</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>5</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.028062</td>\n",
              "      <td>0.691017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.878</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>0.019813</td>\n",
              "      <td>0.910812</td>\n",
              "      <td>0.069375</td>\n",
              "      <td>0.157981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>324</td>\n",
              "      <td>10</td>\n",
              "      <td>191</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>9</td>\n",
              "      <td>299</td>\n",
              "      <td>8</td>\n",
              "      <td>322</td>\n",
              "      <td>8</td>\n",
              "      <td>315</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.780305</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.041278</td>\n",
              "      <td>0.950944</td>\n",
              "      <td>0.007778</td>\n",
              "      <td>-0.145089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  count_title_sentences  ...  text_pos  text_compound\n",
              "0      2                      1  ...  0.038000      -0.100422\n",
              "1      3                      1  ...  0.000000       0.000000\n",
              "2      3                      1  ...  0.096111      -0.040711\n",
              "3      3                      1  ...  0.069375       0.157981\n",
              "4      2                      1  ...  0.007778      -0.145089\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kC_03q-OPWww",
        "outputId": "5d947439-0fd1-45c0-f38b-74c8311a84b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df_final_binary = df_final[distilled_features_cols]\n",
        "df_final_binary.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>162</td>\n",
              "      <td>8</td>\n",
              "      <td>250</td>\n",
              "      <td>8</td>\n",
              "      <td>238</td>\n",
              "      <td>7</td>\n",
              "      <td>249</td>\n",
              "      <td>7</td>\n",
              "      <td>246</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134715</td>\n",
              "      <td>0.990605</td>\n",
              "      <td>0.649405</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6369</td>\n",
              "      <td>0.091077</td>\n",
              "      <td>0.881462</td>\n",
              "      <td>0.027538</td>\n",
              "      <td>-0.248646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>422</td>\n",
              "      <td>11</td>\n",
              "      <td>266</td>\n",
              "      <td>10</td>\n",
              "      <td>421</td>\n",
              "      <td>10</td>\n",
              "      <td>401</td>\n",
              "      <td>9</td>\n",
              "      <td>420</td>\n",
              "      <td>9</td>\n",
              "      <td>418</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>0.271865</td>\n",
              "      <td>0.806206</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.056150</td>\n",
              "      <td>0.804200</td>\n",
              "      <td>0.139650</td>\n",
              "      <td>0.346365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>225</td>\n",
              "      <td>9</td>\n",
              "      <td>165</td>\n",
              "      <td>8</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>214</td>\n",
              "      <td>7</td>\n",
              "      <td>223</td>\n",
              "      <td>7</td>\n",
              "      <td>222</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187350</td>\n",
              "      <td>0.710030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.057059</td>\n",
              "      <td>0.823118</td>\n",
              "      <td>0.119824</td>\n",
              "      <td>0.199129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.187610</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "      <td>434</td>\n",
              "      <td>10</td>\n",
              "      <td>306</td>\n",
              "      <td>9</td>\n",
              "      <td>433</td>\n",
              "      <td>9</td>\n",
              "      <td>426</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>8</td>\n",
              "      <td>432</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224354</td>\n",
              "      <td>0.999797</td>\n",
              "      <td>0.783566</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.4754</td>\n",
              "      <td>0.082300</td>\n",
              "      <td>0.804600</td>\n",
              "      <td>0.113120</td>\n",
              "      <td>0.058830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  count_title_sentences  ...  text_pos  text_compound\n",
              "0      0                      1  ...  0.027538      -0.248646\n",
              "1      1                      1  ...  0.139650       0.346365\n",
              "2      1                      1  ...  0.119824       0.199129\n",
              "3      1                      1  ...  0.000000       0.000000\n",
              "4      1                      1  ...  0.113120       0.058830\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k4MGqEMdWFp",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hycl1WkYdWl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, accuracy_score, \\\n",
        "    classification_report, precision_recall_curve, roc_curve, auc, average_precision_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split, cross_val_predict\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxYy8dv1dfaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_classifiers = {\n",
        "    \"LogisticRegression\": (LogisticRegression(max_iter=10000, solver='lbfgs')),\n",
        "    \"KNearest\": (KNeighborsClassifier()),\n",
        "    \"Support Vector Classifier\": (SVC()),\n",
        "    \"DecisionTreeClassifier\": (DecisionTreeClassifier()),\n",
        "    \"Naive Bayes\": (GaussianNB()),\n",
        "    \"Random forests\": (RandomForestClassifier()),\n",
        "    \"XGBoost Classifier\": (XGBClassifier()),\n",
        "    \"GradientBoostingClassifier\": (GradientBoostingClassifier()),\n",
        "    \"MLP Classifier\": (MLPClassifier())\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1UFVBUedmw8",
        "colab": {}
      },
      "source": [
        "binary_classifiers = {\n",
        "    \"LogisticRegression\": (LogisticRegression(max_iter=10000, solver='lbfgs')),\n",
        "    \"KNearest\": (KNeighborsClassifier()),\n",
        "    \"Support Vector Classifier\": (SVC()),\n",
        "    \"DecisionTreeClassifier\": (DecisionTreeClassifier()),\n",
        "    \"Naive Bayes\": (GaussianNB()),\n",
        "    \"Random forests\": (RandomForestClassifier()),\n",
        "    \"XGBoost Classifier\": (XGBClassifier()),\n",
        "    \"GradientBoostingClassifier\": (GradientBoostingClassifier()),\n",
        "    \"MLP Classifier\": (MLPClassifier())\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KIohUnHdjEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "\n",
        "def run_all_models(folds, X_final_train, y_final_train, X_final_test, y_final_test, classifiers):\n",
        "    for classifier_name, classifier in classifiers.items():\n",
        "        k_fold = StratifiedKFold(n_splits=folds, random_state=100, shuffle=True)\n",
        "        cross_val_scores = []\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        roc_auc_scores = []\n",
        "        f1_scores = []\n",
        "        for train_index, test_index in k_fold.split(X_final_train, y_final_train):\n",
        "            X_train, X_test = pd.DataFrame(data=X_final_train, index=train_index), pd.DataFrame(data=X_final_train, index=test_index)\n",
        "            y_train, y_test = pd.DataFrame(data=y_final_train, index=train_index), pd.DataFrame(data=y_final_train, index=test_index)\n",
        "            model = classifier\n",
        "\n",
        "            model.fit(X_train, y_train.values.ravel())\n",
        "            scores = cross_val_score(model, X_train, y_train.values.ravel(), cv=5)\n",
        "            cross_val_scores.append(scores)\n",
        "            y_pred = model.predict(X_test)\n",
        "            precision_scores.append(precision_score(y_test, y_pred, average=None))\n",
        "            recall_scores.append(recall_score(y_test, y_pred, average=None))\n",
        "            # roc_auc_scores.append(roc_auc_score(y_test, model.predict_proba(X_test), average='macro', multi_class='ovo'))\n",
        "            f1_scores.append(f1_score(y_test, y_pred, average=None))\n",
        "\n",
        "        print('============================= {} ============================='.format(classifier_name))\n",
        "        print('Mean cross validation score: {}'.format(np.array([cross_val_scores]).mean()))\n",
        "        print('Mean precision score: {}'.format(np.array([precision_scores]).mean()))\n",
        "        print('Mean Recall score: {}'.format(np.array([recall_scores]).mean()))\n",
        "        # print('Mean ROC-AUC score: {}'.format(np.array([roc_auc_scores]).mean()))\n",
        "        print('Mean F1 score: {}'.format(np.array([f1_scores]).mean()))\n",
        "        print('******* Real test dataset metrics *******')\n",
        "        y_final_pred = model.predict((X_final_test))\n",
        "        print('Accuracy score for the real test set:\\n', accuracy_score(y_final_test, y_final_pred))\n",
        "        print('confusion matrix for the real test set:\\n', confusion_matrix(y_final_test, y_final_pred))\n",
        "        print('Classification report for the real test set:\\n', classification_report(y_final_test, y_final_pred))\n",
        "        # if classifier_name != 'Support Vector Classifier':\n",
        "        #     y_final_pred_prob = model.predict_proba(X_test)\n",
        "        #     plot_auc_roc_curve(y_test, y_final_pred_prob[:, 1], classifier_name)\n",
        "        #     plot_precision_recall_curve(y_test, y_final_pred_prob[:, 1], classifier_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJue4skdoBR",
        "colab_type": "text"
      },
      "source": [
        "## AUC-ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXSUFF29dlEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_auc_roc_curve(y_test, y_pred, name):\n",
        "    fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC\n",
        "    plt.title('ROC for {}'.format(name))\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([-0.1, 1.0])\n",
        "    plt.ylim([-0.1, 1.01])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW4oNmwhdsyN",
        "colab_type": "text"
      },
      "source": [
        "## Precision Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fNcPlK5dnEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_precision_recall_curve(y_test, y_pred_prob, name):\n",
        "    # Generate precision recall curve values: precision, recall, thresholds\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test.to_numpy(), y_pred_prob)\n",
        "\n",
        "    # Plot Precision Recall curve\n",
        "    plt.plot(recall, precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    avg_precision_score = average_precision_score(y_test, y_pred_prob)\n",
        "    plt.title('PRC for {} - avg precision score: {}'.format(name, str(avg_precision_score)))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urVe9bQVXuF4",
        "colab_type": "text"
      },
      "source": [
        "*italicized text*### Model training on Multiclass classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7uSW1Hdxio",
        "colab_type": "code",
        "outputId": "42ed168f-9334-40a9-88e3-0e6796ba55ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_final_multi.fillna(0, inplace=True)\n",
        "\n",
        "y = df_final_multi['label']\n",
        "X = df_final_multi.drop(['label'], axis=1)\n",
        "\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_final_train, X_final_test, y_final_train, y_final_test = train_test_split(X, y, test_size=0.20, random_state=100, stratify=y)\n",
        "X_final_train.reset_index(drop=True, inplace=True)\n",
        "X_final_test.reset_index(drop=True, inplace=True)\n",
        "y_final_train.reset_index(drop=True, inplace=True)\n",
        "y_final_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "run_all_models(5, X_final_train, y_final_train, X_final_test, y_final_test, multi_classifiers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= LogisticRegression =============================\n",
            "Mean cross validation score: 0.8664948453608248\n",
            "Mean precision score: 0.43838629534553314\n",
            "Mean Recall score: 0.4608374384236453\n",
            "Mean F1 score: 0.4413434060644761\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8807947019867549\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0  10   1]\n",
            " [  2   0   1   0]\n",
            " [  0   0  24   3]\n",
            " [  0   0   1 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.67      0.89      0.76        27\n",
            "           3       0.96      0.99      0.98       110\n",
            "\n",
            "    accuracy                           0.88       151\n",
            "   macro avg       0.41      0.47      0.43       151\n",
            "weighted avg       0.82      0.88      0.85       151\n",
            "\n",
            "============================= KNearest =============================\n",
            "Mean cross validation score: 0.7078049828178693\n",
            "Mean precision score: 0.4371315881356909\n",
            "Mean Recall score: 0.3306684455391352\n",
            "Mean F1 score: 0.3389084660208084\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7019867549668874\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0   1  10]\n",
            " [  1   0   0   2]\n",
            " [  0   0   6  21]\n",
            " [  0   0  10 100]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.35      0.22      0.27        27\n",
            "           3       0.75      0.91      0.82       110\n",
            "\n",
            "    accuracy                           0.70       151\n",
            "   macro avg       0.28      0.28      0.27       151\n",
            "weighted avg       0.61      0.70      0.65       151\n",
            "\n",
            "============================= Support Vector Classifier =============================\n",
            "Mean cross validation score: 0.7243127147766323\n",
            "Mean precision score: 0.18117768595041323\n",
            "Mean Recall score: 0.25\n",
            "Mean F1 score: 0.21009542040720533\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7284768211920529\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0   0  11]\n",
            " [  0   0   0   3]\n",
            " [  0   0   0  27]\n",
            " [  0   0   0 110]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.00      0.00      0.00        27\n",
            "           3       0.73      1.00      0.84       110\n",
            "\n",
            "    accuracy                           0.73       151\n",
            "   macro avg       0.18      0.25      0.21       151\n",
            "weighted avg       0.53      0.73      0.61       151\n",
            "\n",
            "============================= DecisionTreeClassifier =============================\n",
            "Mean cross validation score: 0.8026116838487973\n",
            "Mean precision score: 0.4857734386954336\n",
            "Mean Recall score: 0.48805791909240187\n",
            "Mean F1 score: 0.48001225859393326\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8145695364238411\n",
            "confusion matrix for the real test set:\n",
            " [[  2   0   6   3]\n",
            " [  2   0   1   0]\n",
            " [  6   1  16   4]\n",
            " [  1   0   4 105]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.18      0.18        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.59      0.59      0.59        27\n",
            "           3       0.94      0.95      0.95       110\n",
            "\n",
            "    accuracy                           0.81       151\n",
            "   macro avg       0.43      0.43      0.43       151\n",
            "weighted avg       0.80      0.81      0.81       151\n",
            "\n",
            "============================= Naive Bayes =============================\n",
            "Mean cross validation score: 0.7487800687285222\n",
            "Mean precision score: 0.48434019789132704\n",
            "Mean Recall score: 0.5620527939493457\n",
            "Mean F1 score: 0.4370638001229459\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7748344370860927\n",
            "confusion matrix for the real test set:\n",
            " [[  2   5   4   0]\n",
            " [  1   1   1   0]\n",
            " [  1  11  13   2]\n",
            " [  1   6   2 101]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.18      0.25        11\n",
            "           1       0.04      0.33      0.08         3\n",
            "           2       0.65      0.48      0.55        27\n",
            "           3       0.98      0.92      0.95       110\n",
            "\n",
            "    accuracy                           0.77       151\n",
            "   macro avg       0.52      0.48      0.46       151\n",
            "weighted avg       0.86      0.77      0.81       151\n",
            "\n",
            "============================= Random forests =============================\n",
            "Mean cross validation score: 0.8582560137457044\n",
            "Mean precision score: 0.4640138683180723\n",
            "Mean Recall score: 0.4708187167238891\n",
            "Mean F1 score: 0.4555409411508971\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8543046357615894\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0  10   1]\n",
            " [  1   0   2   0]\n",
            " [  4   0  20   3]\n",
            " [  0   0   1 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.61      0.74      0.67        27\n",
            "           3       0.96      0.99      0.98       110\n",
            "\n",
            "    accuracy                           0.85       151\n",
            "   macro avg       0.39      0.43      0.41       151\n",
            "weighted avg       0.81      0.85      0.83       151\n",
            "\n",
            "============================= XGBoost Classifier =============================\n",
            "Mean cross validation score: 0.847426975945017\n",
            "Mean precision score: 0.5241201373751939\n",
            "Mean Recall score: 0.4958868239040653\n",
            "Mean F1 score: 0.4934807968660818\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8741721854304636\n",
            "confusion matrix for the real test set:\n",
            " [[  2   0   8   1]\n",
            " [  1   0   2   0]\n",
            " [  3   0  21   3]\n",
            " [  0   0   1 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.18      0.24        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.66      0.78      0.71        27\n",
            "           3       0.96      0.99      0.98       110\n",
            "\n",
            "    accuracy                           0.87       151\n",
            "   macro avg       0.49      0.49      0.48       151\n",
            "weighted avg       0.84      0.87      0.86       151\n",
            "\n",
            "============================= GradientBoostingClassifier =============================\n",
            "Mean cross validation score: 0.8511383161512027\n",
            "Mean precision score: 0.5669530201113006\n",
            "Mean Recall score: 0.5147114619097378\n",
            "Mean F1 score: 0.5202761162465608\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8543046357615894\n",
            "confusion matrix for the real test set:\n",
            " [[  2   0   8   1]\n",
            " [  1   0   2   0]\n",
            " [  6   0  18   3]\n",
            " [  0   0   1 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.18      0.20        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.62      0.67      0.64        27\n",
            "           3       0.96      0.99      0.98       110\n",
            "\n",
            "    accuracy                           0.85       151\n",
            "   macro avg       0.45      0.46      0.46       151\n",
            "weighted avg       0.83      0.85      0.84       151\n",
            "\n",
            "============================= MLP Classifier =============================\n",
            "Mean cross validation score: 0.8300816151202749\n",
            "Mean precision score: 0.4076930343640838\n",
            "Mean Recall score: 0.4116687192118227\n",
            "Mean F1 score: 0.4011129873621191\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.847682119205298\n",
            "confusion matrix for the real test set:\n",
            " [[  1   0   9   1]\n",
            " [  1   0   1   1]\n",
            " [  4   0  18   5]\n",
            " [  0   0   1 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.09      0.12        11\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.62      0.67      0.64        27\n",
            "           3       0.94      0.99      0.96       110\n",
            "\n",
            "    accuracy                           0.85       151\n",
            "   macro avg       0.43      0.44      0.43       151\n",
            "weighted avg       0.81      0.85      0.83       151\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWWSRTASicfB",
        "colab_type": "text"
      },
      "source": [
        "### Multiclass Classification Results\n",
        "\n",
        "As we can see above, Random Forest and XgBoost models perfomed well for multi classfication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UmJz9CgjYD63"
      },
      "source": [
        "### Model training on Binary classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_H42gWpYD65",
        "outputId": "c417ae6c-631f-468e-81d0-7344741b4b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_final_binary.fillna(0, inplace=True)\n",
        "\n",
        "y = df_final_binary['label']\n",
        "X = df_final_binary.drop(['label'], axis=1)\n",
        "\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_final_train, X_final_test, y_final_train, y_final_test = train_test_split(X, y, test_size=0.20, random_state=100, stratify=y)\n",
        "X_final_train.reset_index(drop=True, inplace=True)\n",
        "X_final_test.reset_index(drop=True, inplace=True)\n",
        "y_final_train.reset_index(drop=True, inplace=True)\n",
        "y_final_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "run_all_models(5, X_final_train, y_final_train, X_final_test, y_final_test, binary_classifiers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= LogisticRegression =============================\n",
            "Mean cross validation score: 0.7191208791208791\n",
            "Mean precision score: 0.7238496439257158\n",
            "Mean Recall score: 0.721692161438078\n",
            "Mean F1 score: 0.7208227998356921\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6690140845070423\n",
            "confusion matrix for the real test set:\n",
            " [[50 20]\n",
            " [27 45]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.71      0.68        70\n",
            "           1       0.69      0.62      0.66        72\n",
            "\n",
            "    accuracy                           0.67       142\n",
            "   macro avg       0.67      0.67      0.67       142\n",
            "weighted avg       0.67      0.67      0.67       142\n",
            "\n",
            "============================= KNearest =============================\n",
            "Mean cross validation score: 0.6097680097680098\n",
            "Mean precision score: 0.6283626907549988\n",
            "Mean Recall score: 0.618265491314493\n",
            "Mean F1 score: 0.6104931114729188\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6126760563380281\n",
            "confusion matrix for the real test set:\n",
            " [[56 14]\n",
            " [41 31]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.80      0.67        70\n",
            "           1       0.69      0.43      0.53        72\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.63      0.62      0.60       142\n",
            "weighted avg       0.63      0.61      0.60       142\n",
            "\n",
            "============================= Support Vector Classifier =============================\n",
            "Mean cross validation score: 0.6107252747252747\n",
            "Mean precision score: 0.6264587573364565\n",
            "Mean Recall score: 0.6085115806758274\n",
            "Mean F1 score: 0.5959239147617903\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6056338028169014\n",
            "confusion matrix for the real test set:\n",
            " [[28 42]\n",
            " [14 58]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.40      0.50        70\n",
            "           1       0.58      0.81      0.67        72\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.62      0.60      0.59       142\n",
            "weighted avg       0.62      0.61      0.59       142\n",
            "\n",
            "============================= DecisionTreeClassifier =============================\n",
            "Mean cross validation score: 0.6935482295482295\n",
            "Mean precision score: 0.703827828171933\n",
            "Mean Recall score: 0.7037086250108029\n",
            "Mean F1 score: 0.7036805520955924\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6126760563380281\n",
            "confusion matrix for the real test set:\n",
            " [[44 26]\n",
            " [29 43]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.63      0.62        70\n",
            "           1       0.62      0.60      0.61        72\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.61      0.61      0.61       142\n",
            "weighted avg       0.61      0.61      0.61       142\n",
            "\n",
            "============================= Naive Bayes =============================\n",
            "Mean cross validation score: 0.7208302808302808\n",
            "Mean precision score: 0.7461868913161707\n",
            "Mean Recall score: 0.7364369976665802\n",
            "Mean F1 score: 0.7328002196407398\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6549295774647887\n",
            "confusion matrix for the real test set:\n",
            " [[57 13]\n",
            " [36 36]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.81      0.70        70\n",
            "           1       0.73      0.50      0.60        72\n",
            "\n",
            "    accuracy                           0.65       142\n",
            "   macro avg       0.67      0.66      0.65       142\n",
            "weighted avg       0.67      0.65      0.65       142\n",
            "\n",
            "============================= Random forests =============================\n",
            "Mean cross validation score: 0.7865787545787546\n",
            "Mean precision score: 0.8140404441448668\n",
            "Mean Recall score: 0.8132065076484315\n",
            "Mean F1 score: 0.8128816049314052\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.704225352112676\n",
            "confusion matrix for the real test set:\n",
            " [[54 16]\n",
            " [26 46]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.77      0.72        70\n",
            "           1       0.74      0.64      0.69        72\n",
            "\n",
            "    accuracy                           0.70       142\n",
            "   macro avg       0.71      0.71      0.70       142\n",
            "weighted avg       0.71      0.70      0.70       142\n",
            "\n",
            "============================= XGBoost Classifier =============================\n",
            "Mean cross validation score: 0.7716385836385835\n",
            "Mean precision score: 0.7988679151906575\n",
            "Mean Recall score: 0.7973835450695704\n",
            "Mean F1 score: 0.7969361774499225\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.704225352112676\n",
            "confusion matrix for the real test set:\n",
            " [[54 16]\n",
            " [26 46]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.77      0.72        70\n",
            "           1       0.74      0.64      0.69        72\n",
            "\n",
            "    accuracy                           0.70       142\n",
            "   macro avg       0.71      0.71      0.70       142\n",
            "weighted avg       0.71      0.70      0.70       142\n",
            "\n",
            "============================= GradientBoostingClassifier =============================\n",
            "Mean cross validation score: 0.7703052503052501\n",
            "Mean precision score: 0.7999915266422424\n",
            "Mean Recall score: 0.7992005876760868\n",
            "Mean F1 score: 0.7988623619912648\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7394366197183099\n",
            "confusion matrix for the real test set:\n",
            " [[58 12]\n",
            " [25 47]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.83      0.76        70\n",
            "           1       0.80      0.65      0.72        72\n",
            "\n",
            "    accuracy                           0.74       142\n",
            "   macro avg       0.75      0.74      0.74       142\n",
            "weighted avg       0.75      0.74      0.74       142\n",
            "\n",
            "============================= MLP Classifier =============================\n",
            "Mean cross validation score: 0.6163956043956044\n",
            "Mean precision score: 0.6492135935723433\n",
            "Mean Recall score: 0.6402579725175006\n",
            "Mean F1 score: 0.6346166420215069\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.5633802816901409\n",
            "confusion matrix for the real test set:\n",
            " [[30 40]\n",
            " [22 50]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.43      0.49        70\n",
            "           1       0.56      0.69      0.62        72\n",
            "\n",
            "    accuracy                           0.56       142\n",
            "   macro avg       0.57      0.56      0.55       142\n",
            "weighted avg       0.57      0.56      0.56       142\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zl14ZRgYi5kU"
      },
      "source": [
        "### Binary Classification Results\n",
        "\n",
        "As we can see above, Gradient Boost Classifierperfomed well for binary classfication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65xaqcWv46T",
        "colab_type": "text"
      },
      "source": [
        "## Prediction and Testing\n",
        "Lets create methods that take title and text, generate all features required for the model and predict the multi class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uSfNzhxwMAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_distilled_dataset(title, text):\n",
        "  data = {'title': [title], 'text': [text]}\n",
        "  df_test = pd.DataFrame(data)\n",
        "\n",
        "\t# Add the cleaned columns\n",
        "  df_test['cleaned_title'] = df_test[\"title\"].map(lambda x: cleaning(x))\n",
        "  df_test['cleaned_text'] = df_test[\"text\"].map(lambda x: cleaning(x))\n",
        "\n",
        "\t# Add sentence count columns\n",
        "  count_sentences(df_test)\n",
        "\n",
        "\t# Add ngram features\n",
        "  generate_ngram_features(df_test)\n",
        "\n",
        "\t# Add common ngram counts\n",
        "  common_ngrams_in_text(df_test)\n",
        "\n",
        "\t# Add TF_IDF and similarity scores\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf_similarity_score(df_test)\n",
        "\n",
        "\t# Add word2vec similarity scores\n",
        "  word2vec_similarity_score(df_test)\n",
        "\n",
        "\t# Add Topic Similarity score\n",
        "  topic_similarity_score(df_test, title_tfidf_vectors, text_tfidf_vectors)\n",
        "\n",
        "\t# Add Sentiment scores\n",
        "  df_test = title_sentiment(df_test)\n",
        "  df_test = text_sentiment(df_test)\n",
        "\n",
        "\t# Distill features\n",
        "  X_cols = [x for i,x in enumerate(distilled_features_cols) if x!='label']\n",
        "  return df_test[X_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztX7Sc4dbhx5",
        "colab_type": "text"
      },
      "source": [
        "## Pickle the models\n",
        "\n",
        "Pickling the trained models so that they can be loaded into the the notebook that has other feature analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rAzNiSgbhGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "multi_class_model = multi_classifiers['Random forests']\n",
        "binary_class_model = binary_classifiers['GradientBoostingClassifier']\n",
        "\n",
        "\n",
        "multi_class_model_path = '/content/drive/Shared drives/SheCodes/MLSpring2020/shecodes_employee_attrition/Alternus Vera Sprint 4/Models/multi_class_model.pickle'\n",
        "binary_class_model_path = '/content/drive/Shared drives/SheCodes/MLSpring2020/shecodes_employee_attrition/Alternus Vera Sprint 4/Models/binary_class_model.pickle'\n",
        "\n",
        "\n",
        "pickle.dump(multi_class_model, open(multi_class_model_path, 'wb'))\n",
        "pickle.dump(binary_class_model, open(binary_class_model_path, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8NC6gTEf4MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_class_model = pickle.load(open(multi_class_model_path, \"rb\"))\n",
        "binary_class_model = pickle.load(open(binary_class_model_path, \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmjRwnG8G3cK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_multiclass(title, text):\n",
        "  df_test = get_distilled_dataset(title, text)\n",
        "  return multi_classifiers['Random forests'].predict(df_test), multi_classifiers['Random forests'].predict_proba(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JF0P_BZmG9Wd",
        "colab": {}
      },
      "source": [
        "def predict_binaryclass(title, text):\n",
        "  df_test = get_distilled_dataset(title, text)\n",
        "  return binary_classifiers['GradientBoostingClassifier'].predict(df_test), binary_classifiers['GradientBoostingClassifier'].predict_proba(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqqthy1rIfxq",
        "colab_type": "text"
      },
      "source": [
        "My Polynomial equation gives 60%, 40% weights to the accuracies respectively given by multi label classification predicted by Random Forest and binary classification predicted by Gradient Boost Classifer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRFc7Ma6JV-o",
        "colab_type": "text"
      },
      "source": [
        "For generating the fakeness score in the range of 0 to 1, where 1 denotes the news is absolutely fake, I am considering `Agrees` and `Discusses` categories from the Fake News Dataset as True classification. Remaining labels `Disagrees` and `Unrelated` are considered fake. \n",
        "\n",
        "If prediction agrees, probability of being True is higher as many features might have correlated. So I am giving weight of 40% to `Agrees`, 20% to `Discusses`, 30% to `Disagrees` and 10% to `Unrelated`.\n",
        "\n",
        "So My polynomial would be something like below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZoUIcqgOB3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Shecodes_getTitleVsBodyScore(title, text):\n",
        "  prediction_multi, prediction_multi_prob = predict_multiclass(title, text)\n",
        "  print (prediction_multi)\n",
        "  print (\"Multi label predicted classification: {}\".format(le.inverse_transform(prediction_multi)[0]))\n",
        "  print (\"Multi label prediction probabilities {} : {}\".format(le.inverse_transform([0,1,2,3]), prediction_multi_prob))\n",
        "\n",
        "  prediction_binary, prediction_binary_prob = predict_binaryclass(title, text)\n",
        "  print (prediction_binary)\n",
        "  print (\"Binary predicted classification: {}\".format(\"Fake\" if prediction_binary[0] == 1 else \"True\"))\n",
        "  print (\"Binary label prediction probabilities ['True', 'Fake']: {}\".format(prediction_binary_prob))\n",
        "  print (prediction_multi)\n",
        "\n",
        "  multiclass_truth_score = (prediction_multi_prob[0][0] * 0.6 + prediction_multi_prob[0][2] + 0.4)\n",
        "  multiclass_fake_score = (prediction_multi_prob[0][1] * 0.6 + prediction_multi_prob[0][3] + 0.4)\n",
        "\n",
        "  binaryclass_truth_score = (prediction_binary_prob[0][0] * 0.5)\n",
        "  binaryclass_fake_score = (prediction_binary_prob[0][1] * 0.5)\n",
        "\n",
        "  overall_truth_score = (0.6 * multiclass_truth_score) + (0.4 * binaryclass_truth_score)\n",
        "  overall_fakeness_score = (0.6 * multiclass_fake_score) + (0.4 * binaryclass_fake_score)\n",
        "\n",
        "  if (overall_truth_score > overall_fakeness_score):\n",
        "    if (overall_truth_score > 1):\n",
        "      overall_truth_score = 1\n",
        "    print (\"Overall Fakeness Score: {}\", 1 - overall_truth_score)\n",
        "    return 1 - overall_truth_score\n",
        "  else:\n",
        "    if (overall_fakeness_score > 1):\n",
        "      overall_fakeness_score = 1\n",
        "    print (\"Overall Fakeness Score: {}\", overall_fakeness_score)\n",
        "    return overall_fakeness_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGjX-t-MH2Oz",
        "colab_type": "code",
        "outputId": "d2819d15-2e69-49a2-ed97-55b754b3e166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"A Russian Guy Says His Justin Bieber Ringtone Saved Him From A Bear Attack\", \n",
        "  \"A bereaved Afghan mother took revenge on the Taliban after watching them kill her son in an ambush. Reza Gul killed 25 Taliban fighters and injured five others in a seven-hour gunbattle in Farah province. Gul, who was joined by her daughter and daughter in-law, engaged the Taliban using AK-47s and grenades, despitenever before having used a weapon. The embattled mother told Tolo news, a 24-hour Afghan news broadcaster, she was awakened by shots early Tuesday. After seeing that her son had been killed, Gul and the other two women fought back. “I couldn't stop myself and picked up a weapon,” Gul told Tolo News. “I went to the check post and began shooting back.” Seema, her daughter-in-law, added: “The fighting was intensified when we reached the battlefield along with light and heavy weapons. We were committed to fight until the last bullet.” Gul said that the battlefield was covered in Talib fighters after the deadly exchange ended. While the Taliban have not publicly commented on the incident, the Afghan government labeled it a symbol of a public uprising against the Taliban. Taliban and other groups have regained large swathes of the country as U.S. and NATO forces slowly pull out troops after 14 years of war. The Taliban have targeted government and foreign infrastructure as the group attempts to claw back power it lost in 2001. While the Taliban have made key gains in rural regions, members continue to employ suicide bomber tactics in well protected towns and cities. Earlier this week, 50 people were killed after a suicide bomber detonated a vest during a volleyball competition in Yahyakahil, Paktika province. That particular attack prompted President Ashraf Ghani to order a complete overview of the country’s defense forces and to rethink the ban on nighttime raids, which were outlawed by his predecessor, Hamid Karzai.\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"Donald Trump Sends Out Embarrassing New Year‚Äôs Eve Message; This is Disturbing\", \n",
        "  \"Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"Bad News For Trump ‚Äî Mitch McConnell Says No To Repealing Obamacare In 2018\", \n",
        "  \"Republicans have had seven years to come up with a viable replacement for Obamacare but they failed miserably. After taking a victory lap for gifting the wealthy with a tax break on Wednesday, Donald Trump looked at the cameras and said,  We have essentially repealed Obamacare and we will come up with something that will be much better. Obamacare has been repealed in this bill,  he added. Well, like most things Trump says, that s just not true. But, if the former reality show star could have done that in order to eradicate former President Obama s signature legislation, he would have and without offering an alternative.Senate Majority Leader Mitch McConnell told NPR that  This has not been a very bipartisan year. I hope in the new year, we re going to pivot here and become more cooperative. An Obamacare repeal in 2018 is DOA. Well, we obviously were unable to completely repeal and replace with a 52-48 Senate,  the Kentucky Republican said.  We ll have to take a look at what that looks like with a 51-49 Senate. But I think we ll probably move on to other issues. NPR reports:McConnell hopes to focus instead on stabilizing the insurance marketplaces to keep premiums from skyrocketing in the early months of 2018, a promise he made to moderate Republican Sen. Susan Collins of Maine to get her support for the tax bill.On top of that McConnell broke with House Speaker Paul Ryan, R-Wis., on the approach to paring back spending on programs like Medicaid and food stamps. McConnell told NPR he is  not interested  in using Senate budget rules to allow Republicans to cut entitlements without consultation with Democrats. I think entitlement changes, to be sustained, almost always have to be bipartisan,  McConnell said.  The House may have a different agenda. If our Democratic friends in the Senate want to join us to tackle any kind of entitlement reform. I d be happy to take a look at it. This is coming from Mitch McConnell. He knows Donald Trump is destroying the GOP. It doesn t matter, Sen. McConnell. We still recall him saying that his  number one priority is making sure president Obama s a one-term president. Well, we re hoping that Trump doesn t last a full term. Funny how that works.Photo by Chip Somodevilla/Getty Images\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \" Judge Who Barred A Mom From Seeing Her Baby For A Year Over Unpaid Fees Resigns\", \n",
        "  \"With all the recent talk of sentencing reform and unfair bail across America, sometimes what it takes is an outrageous case in a small town somewhere to light a fire under activists to demand a change. And even though the resignation of a judge in Pearl, Mississippi, a suburb of the capital with a meager 25,000 residents, isn t really anyone s idea of a revolution, it probably means a lot to the mother at the center of the case.Judge John Shirley of the now-dissolved Pearl Youth Court decided back in August of 2016 that some unpaid court fees were enough to separate  Mother A,  a resident of nearby Jackson, from her baby for a period of 14 months. The unidentified mom and a friend were driving through Pearl one day looking for work when they were pulled over by a police officer who discovered that both women had misdemeanor warrants for minor offenses.Honestly, the story begins with that officer, who is also unidentified in the report from the Clarion-Ledger, the news outlet that first reported the judge s resignation. So much begins with a cop s interaction with a young person   will they detain them? Will they give them a warning? Initial contact with law enforcement often determines the course of a young person s life, and most police are totally aware of that fact. Mother A  had her baby with her that day, and although the woman s grandmother came to the scene immediately to collect the baby, that officer decided to change the young woman s life by requiring her to bring the child before the Pearl Youth Court.If it seems like I m being too hard on the cop, let me assure you, I ve saved most of my hate for Judge Shirley. How an arbiter of justice thinks ordering a young mother to stay away from her newborn baby is anything approaching justice is bewildering to me. But that s just what Judge Shirley did, remanding the infant to the custody of the grandmother and imposing a goddamn NO CONTACT ORDER between Mother A and her baby until the poor (as in, without money) young woman could pay off her court fees.Not only could she not see her child, she couldn t see her grandmother while the baby was present. It was just to be cruel. That s the only explanation.Cliff Johnson, the Director of the Roderick and Solange MacArthur Justice Center at the University of Mississippi School of Law, agreed. As a civil rights lawyer in Mississippi, I am no stranger to injustice, but for a judge to prohibit an impoverished mother from having any contact with her baby until monetary payments are made is shocking and repugnant. Such orders are tantamount to judicial kidnapping. Between two people   one cop and one judge   a young woman s life is forever changed, her idea of justice forever skewed, her trust in authority forever hobbled by the fear of a cruelty that exists only for its own sake.Featured image via judgejohnshirley.com\"\n",
        ")\n",
        "print (score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 328)\n",
            "(1, 328)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[3]\n",
            "Multi label predicted classification: unrelated\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.01 0.01 0.06 0.92]]\n",
            "(1, 328)\n",
            "(1, 328)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[1]\n",
            "Binary predicted classification: Fake\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.39808741 0.60191259]]\n",
            "[3]\n",
            "Overall Fakeness Score: {} 0.9159825177842689\n",
            "0.9159825177842689\n",
            "(1, 375)\n",
            "(1, 375)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.37 0.01 0.47 0.15]]\n",
            "(1, 375)\n",
            "(1, 375)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[1]\n",
            "Binary predicted classification: Fake\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.12787245 0.87212755]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.3192255101875816\n",
            "0.3192255101875816\n",
            "(1, 366)\n",
            "(1, 366)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.35 0.04 0.56 0.05]]\n",
            "(1, 366)\n",
            "(1, 366)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[1]\n",
            "Binary predicted classification: Fake\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.34512148 0.65487852]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.22897570303841064\n",
            "0.22897570303841064\n",
            "(1, 440)\n",
            "(1, 440)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.22 0.01 0.47 0.3 ]]\n",
            "(1, 440)\n",
            "(1, 440)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[0]\n",
            "Binary predicted classification: True\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.59787312 0.40212688]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.27922537555732796\n",
            "0.27922537555732796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyg5-2TeXuXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}